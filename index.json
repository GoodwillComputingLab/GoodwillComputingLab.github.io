[{"authors":["abhay"],"categories":null,"content":"Abhay holds a Bachelors degree in Electronics and Communication Engineering. He is currently doing his Masters in Electrical and Computer Engineering with a concentration in Computer Networks and Security. He has completed a Co-op working for Bosch Thermotechnology. He was a part of their Engineering Team as an Electronics Controls Engineering Intern and hopes to use the knowledge acquired in the field of Computer Networks, Internet of Things and High Performance Computing during this experience to aid him in his research.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ae0b3c283d7fcbe95df9fde3da97c6f4","permalink":"https://goodwillcomputinglab.github.io/author/abhay-potharaju/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/abhay-potharaju/","section":"authors","summary":"Abhay holds a Bachelors degree in Electronics and Communication Engineering. He is currently doing his Masters in Electrical and Computer Engineering with a concentration in Computer Networks and Security. He has completed a Co-op working for Bosch Thermotechnology.","tags":null,"title":"Abhay Potharaju","type":"authors"},{"authors":["amir"],"categories":null,"content":"Abhay holds a Bachelors degree in Electronics and Communication Engineering. He is currently doing his Masters in Electrical and Computer Engineering with a concentration in Computer Networks and Security. He has completed a Co-op working for Bosch Thermotechnology. He was a part of their Engineering Team as an Electronics Controls Engineering Intern and hopes to use the knowledge acquired in the field of Computer Networks, Internet of Things and High Performance Computing during this experience to aid him in his research.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8909b1e61d3153a63abca95c6e07c6d7","permalink":"https://goodwillcomputinglab.github.io/author/amir/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/amir/","section":"authors","summary":"Abhay holds a Bachelors degree in Electronics and Communication Engineering. He is currently doing his Masters in Electrical and Computer Engineering with a concentration in Computer Networks and Security. He has completed a Co-op working for Bosch Thermotechnology.","tags":null,"title":"Amir","type":"authors"},{"authors":["baolin"],"categories":null,"content":"Baolin joined Northeastern University as a Computer Engineering PhD student. He is advised by professor Devesh Tiwari and his research interests include cloud computing, GPU computing and deep learning scheduling. Before joining Northeastern University, he received his Master of Science degree from the University of Texas at Austin and worked as a SoC validation engineer for 2 years.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5218817a48de6ad75049aab67ac9ae3f","permalink":"https://goodwillcomputinglab.github.io/author/baolin-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/baolin-li/","section":"authors","summary":"Baolin joined Northeastern University as a Computer Engineering PhD student. He is advised by professor Devesh Tiwari and his research interests include cloud computing, GPU computing and deep learning scheduling. Before joining Northeastern University, he received his Master of Science degree from the University of Texas at Austin and worked as a SoC validation engineer for 2 years.","tags":null,"title":"Baolin Li","type":"authors"},{"authors":["devesh"],"categories":null,"content":"Devesh’s research focuses on innovating new methods to improve efficiency, scalability, and cost-effectiveness of large-scale parallel computing systems. Before joining Northeastern, Devesh was a staff scientist at the Oak Ridge National Laboratory, a flagship multiprogram science and technology national laboratory of the United States Department of Energy (DOE). Devesh earned his Ph.D. in Electrical and Computer Engineering from North Carolina State University and B.S. degree in Computer Science and Engineering from Indian Institute of Technology (IIT) Kanpur in India.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3fa58640d62aa759edc05d012fd3ca19","permalink":"https://goodwillcomputinglab.github.io/author/devesh-tiwari/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/devesh-tiwari/","section":"authors","summary":"Devesh’s research focuses on innovating new methods to improve efficiency, scalability, and cost-effectiveness of large-scale parallel computing systems. Before joining Northeastern, Devesh was a staff scientist at the Oak Ridge National Laboratory, a flagship multiprogram science and technology national laboratory of the United States Department of Energy (DOE).","tags":null,"title":"Devesh Tiwari","type":"authors"},{"authors":["emily"],"categories":null,"content":"Emily joined Northeastern University as a Computer Engineering PhD student. She works on high-performance computing research, such as large-scale file systems and scalability of systems. She is advised by professor Devesh Tiwari. Prior to joining Northeastern University, she completed her Bachelor\u0026rsquo;s of Science in Mathematics and Computer Science from Florida International University in Miami, Florida.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"48313565f7b4723a61a5862a70b11529","permalink":"https://goodwillcomputinglab.github.io/author/emily/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/emily/","section":"authors","summary":"Emily joined Northeastern University as a Computer Engineering PhD student. She works on high-performance computing research, such as large-scale file systems and scalability of systems. She is advised by professor Devesh Tiwari.","tags":null,"title":"Emily","type":"authors"},{"authors":["richmond"],"categories":null,"content":"Abhay holds a Bachelors degree in Electronics and Communication Engineering. He is currently doing his Masters in Electrical and Computer Engineering with a concentration in Computer Networks and Security. He has completed a Co-op working for Bosch Thermotechnology. He was a part of their Engineering Team as an Electronics Controls Engineering Intern and hopes to use the knowledge acquired in the field of Computer Networks, Internet of Things and High Performance Computing during this experience to aid him in his research.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"277c24383b21ac72ca86b23ee154a303","permalink":"https://goodwillcomputinglab.github.io/author/richmond-liew/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/richmond-liew/","section":"authors","summary":"Abhay holds a Bachelors degree in Electronics and Communication Engineering. He is currently doing his Masters in Electrical and Computer Engineering with a concentration in Computer Networks and Security. He has completed a Co-op working for Bosch Thermotechnology.","tags":null,"title":"Richmond Liew","type":"authors"},{"authors":["rohan"],"categories":null,"content":"Rohan is a Ph.D. candidate in Computer Engineering, advised by Professor Devesh Tiwari. He is interested in high performance computing systems and architecture. His research focusses on performance optimization\nin distributed systems and serverless computing platforms. Prior to joining Northeastern, he did his undergraduate in Electronics and Communication Engineering\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a36473218d1c18bbc57ddacf327f2f16","permalink":"https://goodwillcomputinglab.github.io/author/rohan-basu-roy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/rohan-basu-roy/","section":"authors","summary":"Rohan is a Ph.D. candidate in Computer Engineering, advised by Professor Devesh Tiwari. He is interested in high performance computing systems and architecture. His research focusses on performance optimization\nin distributed systems and serverless computing platforms.","tags":null,"title":"Rohan Basu Roy","type":"authors"},{"authors":["rohin"],"categories":null,"content":"Abhay holds a Bachelors degree in Electronics and Communication Engineering. He is currently doing his Masters in Electrical and Computer Engineering with a concentration in Computer Networks and Security. He has completed a Co-op working for Bosch Thermotechnology. He was a part of their Engineering Team as an Electronics Controls Engineering Intern and hopes to use the knowledge acquired in the field of Computer Networks, Internet of Things and High Performance Computing during this experience to aid him in his research.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d97b9be4eeec85a7a96dabf805726187","permalink":"https://goodwillcomputinglab.github.io/author/rohin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/rohin/","section":"authors","summary":"Abhay holds a Bachelors degree in Electronics and Communication Engineering. He is currently doing his Masters in Electrical and Computer Engineering with a concentration in Computer Networks and Security. He has completed a Co-op working for Bosch Thermotechnology.","tags":null,"title":"Rohin","type":"authors"},{"authors":["tirthak"],"categories":null,"content":"Tirthak is a Computer Engineering Ph.D. Candidate at Northeastern University. He works with Professor Devesh Tiwari on projects related to large-scale high-performance computing (HPC). His research mainly relates to resource management and optimization in large-scale HPC computational and storage sub-systems. He is especially interested in exploring the trade-offs between performance, fairness, energy-efficiency, and resiliency. He is a recipient of the NSERC (Canada’s NSF) Alexander Graham Bell Canada Graduate Scholarship (CGS D-3) award. Website: http://ece.neu.edu/~tirthak.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d696fcbe48dc099efe5faa0436a8d4c8","permalink":"https://goodwillcomputinglab.github.io/author/tirthak-patel/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tirthak-patel/","section":"authors","summary":"Tirthak is a Computer Engineering Ph.D. Candidate at Northeastern University. He works with Professor Devesh Tiwari on projects related to large-scale high-performance computing (HPC). His research mainly relates to resource management and optimization in large-scale HPC computational and storage sub-systems.","tags":null,"title":"Tirthak Patel","type":"authors"},{"authors":["trinity"],"categories":null,"content":"Abhay holds a Bachelors degree in Electronics and Communication Engineering. He is currently doing his Masters in Electrical and Computer Engineering with a concentration in Computer Networks and Security. He has completed a Co-op working for Bosch Thermotechnology. He was a part of their Engineering Team as an Electronics Controls Engineering Intern and hopes to use the knowledge acquired in the field of Computer Networks, Internet of Things and High Performance Computing during this experience to aid him in his research.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"aedf3af178710d0b8d6156e2b2678861","permalink":"https://goodwillcomputinglab.github.io/author/trinity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/trinity/","section":"authors","summary":"Abhay holds a Bachelors degree in Electronics and Communication Engineering. He is currently doing his Masters in Electrical and Computer Engineering with a concentration in Computer Networks and Security. He has completed a Co-op working for Bosch Thermotechnology.","tags":null,"title":"Trinity","type":"authors"},{"authors":["twinkle"],"categories":null,"content":"Abhay holds a Bachelors degree in Electronics and Communication Engineering. He is currently doing his Masters in Electrical and Computer Engineering with a concentration in Computer Networks and Security. He has completed a Co-op working for Bosch Thermotechnology. He was a part of their Engineering Team as an Electronics Controls Engineering Intern and hopes to use the knowledge acquired in the field of Computer Networks, Internet of Things and High Performance Computing during this experience to aid him in his research.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c21efa2dcc2fb3ed48b49ec374a5944a","permalink":"https://goodwillcomputinglab.github.io/author/twinkle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/twinkle/","section":"authors","summary":"Abhay holds a Bachelors degree in Electronics and Communication Engineering. He is currently doing his Masters in Electrical and Computer Engineering with a concentration in Computer Networks and Security. He has completed a Co-op working for Bosch Thermotechnology.","tags":null,"title":"Twinkle","type":"authors"},{"authors":["xingzhi"],"categories":null,"content":"Abhay holds a Bachelors degree in Electronics and Communication Engineering. He is currently doing his Masters in Electrical and Computer Engineering with a concentration in Computer Networks and Security. He has completed a Co-op working for Bosch Thermotechnology. He was a part of their Engineering Team as an Electronics Controls Engineering Intern and hopes to use the knowledge acquired in the field of Computer Networks, Internet of Things and High Performance Computing during this experience to aid him in his research.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ebb321e385de96f827a79c4cfacc7263","permalink":"https://goodwillcomputinglab.github.io/author/xing-zhi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xing-zhi/","section":"authors","summary":"Abhay holds a Bachelors degree in Electronics and Communication Engineering. He is currently doing his Masters in Electrical and Computer Engineering with a concentration in Computer Networks and Security. He has completed a Co-op working for Bosch Thermotechnology.","tags":null,"title":"Xing Zhi","type":"authors"},{"authors":["Hyogi Sim","Geoffroy Vallée","Youngjae Kim","Sudharshan S. Vazhkudai","Devesh Tiwari","Ali Raza Butt"],"categories":null,"content":"","date":1600646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600646400,"objectID":"85e025106546fa5cb1a9a7eb0bdbef36","permalink":"https://goodwillcomputinglab.github.io/publication/temp4/","publishdate":"2020-09-21T21:05:21.259026Z","relpermalink":"/publication/temp4/","section":"publication","summary":"The need for novel data analysis is urgent in the face of a data deluge from modern applications. Traditional approaches to data analysis incur significant data movement costs, moving data back and forth between the storage system and the processor. Emerging Active Flash devices enable processing on the flash, where the data already resides. An array of such Active Flash devices allows us to revisit how analysis workflows interact with storage systems. By seamlessly blending together the flash storage and data analysis, we create an analysis workflow-aware storage system, AnalyzeThis. Our guiding principle is that analysis-awareness be deeply ingrained in each and every layer of the storage, elevating data analyses as first-class citizens, and transforming AnalyzeThis into a potent analytics-aware appliance. To evaluate the AnalyzeThis system, we have adopted both emulation and simulation approaches. In particular, we have evaluated AnalyzeThis by implementing the AnalyzeThis storage system on top of the Active Flash Array's emulation platform. We have also implemented an event-driven AnalyzeThis simulator, called AnalyzeThisSim, which allows us to address the limitations of the emulation platform, e.g., performance impact of using multi-core SSDs. The results from our emulation and simulation platforms indicate that AnalyzeThis is a viable approach for expediting workflow execution and minimizing data movement.","tags":null,"title":"DISQ: A Novel Quantum Output State Classification Method on IBM Quantum Computers using OpenPulse (Best Paper Finalist)","type":"publication"},{"authors":["Hyogi Sim","Geoffroy Vallée","Youngjae Kim","Sudharshan S. Vazhkudai","Devesh Tiwari","Ali Raza Butt"],"categories":null,"content":"","date":1600646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600646400,"objectID":"520d3838e35b47f4bec89c89aef5e08f","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-journalstpds-temp/","publishdate":"2020-09-21T21:05:21.259026Z","relpermalink":"/publication/dblp-journalstpds-temp/","section":"publication","summary":"The need for novel data analysis is urgent in the face of a data deluge from modern applications. Traditional approaches to data analysis incur significant data movement costs, moving data back and forth between the storage system and the processor. Emerging Active Flash devices enable processing on the flash, where the data already resides. An array of such Active Flash devices allows us to revisit how analysis workflows interact with storage systems. By seamlessly blending together the flash storage and data analysis, we create an analysis workflow-aware storage system, AnalyzeThis. Our guiding principle is that analysis-awareness be deeply ingrained in each and every layer of the storage, elevating data analyses as first-class citizens, and transforming AnalyzeThis into a potent analytics-aware appliance. To evaluate the AnalyzeThis system, we have adopted both emulation and simulation approaches. In particular, we have evaluated AnalyzeThis by implementing the AnalyzeThis storage system on top of the Active Flash Array's emulation platform. We have also implemented an event-driven AnalyzeThis simulator, called AnalyzeThisSim, which allows us to address the limitations of the emulation platform, e.g., performance impact of using multi-core SSDs. The results from our emulation and simulation platforms indicate that AnalyzeThis is a viable approach for expediting workflow execution and minimizing data movement.","tags":null,"title":"Experimental Evaluation of NISQ Quantum Computers: Error Measurement, Characterization, and Implications (Best Paper Finalist, Best Student Paper Finalist)","type":"publication"},{"authors":["Hyogi Sim","Geoffroy Vallée","Youngjae Kim","Sudharshan S. Vazhkudai","Devesh Tiwari","Ali Raza Butt"],"categories":null,"content":"","date":1600646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600646400,"objectID":"832e589d9fdee10e460acf829ad5a235","permalink":"https://goodwillcomputinglab.github.io/publication/temp3/","publishdate":"2020-09-21T21:05:21.259026Z","relpermalink":"/publication/temp3/","section":"publication","summary":"The need for novel data analysis is urgent in the face of a data deluge from modern applications. Traditional approaches to data analysis incur significant data movement costs, moving data back and forth between the storage system and the processor. Emerging Active Flash devices enable processing on the flash, where the data already resides. An array of such Active Flash devices allows us to revisit how analysis workflows interact with storage systems. By seamlessly blending together the flash storage and data analysis, we create an analysis workflow-aware storage system, AnalyzeThis. Our guiding principle is that analysis-awareness be deeply ingrained in each and every layer of the storage, elevating data analyses as first-class citizens, and transforming AnalyzeThis into a potent analytics-aware appliance. To evaluate the AnalyzeThis system, we have adopted both emulation and simulation approaches. In particular, we have evaluated AnalyzeThis by implementing the AnalyzeThis storage system on top of the Active Flash Array's emulation platform. We have also implemented an event-driven AnalyzeThis simulator, called AnalyzeThisSim, which allows us to address the limitations of the emulation platform, e.g., performance impact of using multi-core SSDs. The results from our emulation and simulation platforms indicate that AnalyzeThis is a viable approach for expediting workflow execution and minimizing data movement.","tags":null,"title":"Job Characteristics on Large-Scale Systems: Long-Term Analysis, Quantification and Implications","type":"publication"},{"authors":["Hyogi Sim","Geoffroy Vallée","Youngjae Kim","Sudharshan S. Vazhkudai","Devesh Tiwari","Ali Raza Butt"],"categories":null,"content":"","date":1600646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600646400,"objectID":"916368b0728eea076704abdf6468d560","permalink":"https://goodwillcomputinglab.github.io/publication/temp5/","publishdate":"2020-09-21T21:05:21.259026Z","relpermalink":"/publication/temp5/","section":"publication","summary":"The need for novel data analysis is urgent in the face of a data deluge from modern applications. Traditional approaches to data analysis incur significant data movement costs, moving data back and forth between the storage system and the processor. Emerging Active Flash devices enable processing on the flash, where the data already resides. An array of such Active Flash devices allows us to revisit how analysis workflows interact with storage systems. By seamlessly blending together the flash storage and data analysis, we create an analysis workflow-aware storage system, AnalyzeThis. Our guiding principle is that analysis-awareness be deeply ingrained in each and every layer of the storage, elevating data analyses as first-class citizens, and transforming AnalyzeThis into a potent analytics-aware appliance. To evaluate the AnalyzeThis system, we have adopted both emulation and simulation approaches. In particular, we have evaluated AnalyzeThis by implementing the AnalyzeThis storage system on top of the Active Flash Array's emulation platform. We have also implemented an event-driven AnalyzeThis simulator, called AnalyzeThisSim, which allows us to address the limitations of the emulation platform, e.g., performance impact of using multi-core SSDs. The results from our emulation and simulation platforms indicate that AnalyzeThis is a viable approach for expediting workflow execution and minimizing data movement.","tags":null,"title":"UREQA: Leveraging Operation-Aware Error Rates for Effective Quantum Circuit Mapping on NISQ-Era Quantum Computers","type":"publication"},{"authors":["Hyogi Sim","Geoffroy Vallée","Youngjae Kim","Sudharshan S. Vazhkudai","Devesh Tiwari","Ali Raza Butt"],"categories":null,"content":"","date":1600646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600646400,"objectID":"b9cba8f65f763740823f08a882288aff","permalink":"https://goodwillcomputinglab.github.io/publication/temp2/","publishdate":"2020-09-21T21:05:21.259026Z","relpermalink":"/publication/temp2/","section":"publication","summary":"The need for novel data analysis is urgent in the face of a data deluge from modern applications. Traditional approaches to data analysis incur significant data movement costs, moving data back and forth between the storage system and the processor. Emerging Active Flash devices enable processing on the flash, where the data already resides. An array of such Active Flash devices allows us to revisit how analysis workflows interact with storage systems. By seamlessly blending together the flash storage and data analysis, we create an analysis workflow-aware storage system, AnalyzeThis. Our guiding principle is that analysis-awareness be deeply ingrained in each and every layer of the storage, elevating data analyses as first-class citizens, and transforming AnalyzeThis into a potent analytics-aware appliance. To evaluate the AnalyzeThis system, we have adopted both emulation and simulation approaches. In particular, we have evaluated AnalyzeThis by implementing the AnalyzeThis storage system on top of the Active Flash Array's emulation platform. We have also implemented an event-driven AnalyzeThis simulator, called AnalyzeThisSim, which allows us to address the limitations of the emulation platform, e.g., performance impact of using multi-core SSDs. The results from our emulation and simulation platforms indicate that AnalyzeThis is a viable approach for expediting workflow execution and minimizing data movement.","tags":null,"title":"VERITAS: Accurately Estimating the Correct Output on Noisy Intermediate-Scale Quantum Computers","type":"publication"},{"authors":[],"categories":[],"content":"","date":1598639073,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598639073,"objectID":"8ac024af5501bd078d5ee62351fb4806","permalink":"https://goodwillcomputinglab.github.io/artifacts/post2/","publishdate":"2020-08-28T14:24:33-04:00","relpermalink":"/artifacts/post2/","section":"artifacts","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean consequat turpis lorem, quis aliquet quam facilisis quis. Curabitur consequat consequat eros, quis posuere lorem blandit in. Nulla vel est congue, lobortis ligula ac, finibus risus. Cras eu est leo. Vivamus tempor magna ex, a mollis nulla suscipit sed. Vivamus eu lectus odio. Mauris vel varius leo. Phasellus gravida lacinia sapien convallis tristique. Sed pellentesque in risus et rutrum. Nam lorem tortor, suscipit ut tellus a, consectetur fermentum magna. ","tags":[],"title":"SpaceX Partners with Northeastern","type":"artifacts"},{"authors":[],"categories":[],"content":"","date":1598639073,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598639073,"objectID":"53db69e732b1baa910ad79064f83ab97","permalink":"https://goodwillcomputinglab.github.io/news/post2/","publishdate":"2020-08-28T14:24:33-04:00","relpermalink":"/news/post2/","section":"news","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean consequat turpis lorem, quis aliquet quam facilisis quis. Curabitur consequat consequat eros, quis posuere lorem blandit in. Nulla vel est congue, lobortis ligula ac, finibus risus. Cras eu est leo. Vivamus tempor magna ex, a mollis nulla suscipit sed. Vivamus eu lectus odio. Mauris vel varius leo. Phasellus gravida lacinia sapien convallis tristique. Sed pellentesque in risus et rutrum. Nam lorem tortor, suscipit ut tellus a, consectetur fermentum magna. ","tags":[],"title":"SpaceX Partners with Northeastern","type":"news"},{"authors":[],"categories":[],"content":"","date":1598638830,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598638830,"objectID":"9fc85278183d2c1a04e740c1c864e87d","permalink":"https://goodwillcomputinglab.github.io/research/research-topic-2/","publishdate":"2020-08-28T14:20:30-04:00","relpermalink":"/research/research-topic-2/","section":"research","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec eget ex augue. Sed nec quam dui. Integer ullamcorper turpis id fermentum venenatis. In dictum tellus eu sagittis maximus. Nulla facilisi. Curabitur sodales ligula nec leo pharetra rhoncus. Etiam enim ipsum, ultricies dictum libero sit amet, congue aliquet risus. Sed sed ipsum aliquam, luctus purus auctor, ultricies nisl. Fusce nec arcu id ex sodales egestas. Proin suscipit nunc diam, sit amet mattis dolor posuere eu. Fusce a laoreet diam. Pellentesque malesuada, nisl a interdum condimentum, lacus tortor dictum quam, nec varius sapien leo vel elit. Pellentesque porta sem ac nisi imperdiet mattis.\nFusce hendrerit quam urna, eu porta tortor maximus quis. Vestibulum sed sollicitudin nisi. Nulla sed nisl in mi tempus accumsan. Aenean iaculis nisi mauris. Cras quis sagittis nisi. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer eu magna nec nibh varius venenatis vitae sit amet est. Nam dapibus dui sit amet purus facilisis, vel facilisis nisl tristique. Vestibulum in velit vel lectus rutrum sagittis. Integer et iaculis ipsum, at luctus nibh. Donec sed justo at nunc facilisis lobortis eu laoreet enim. Maecenas et ante vitae nibh feugiat posuere ac vel ante. Aliquam quis nisl rutrum, pharetra felis et, ultricies est. Curabitur nec cursus diam, feugiat pharetra nisi. Phasellus eu velit sed mi luctus interdum at in est. Pellentesque ullamcorper neque in massa fermentum vehicula. Nulla massa eros, efficitur tempus blandit quis, venenatis non arcu. In hac habitasse platea dictumst. ","tags":[],"title":"HPC and Data Center Computing Systems","type":"research"},{"authors":[],"categories":[],"content":"","date":1598638830,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598638830,"objectID":"9e166128ec3c9aee91d8b75f9046a82a","permalink":"https://goodwillcomputinglab.github.io/research/research-topic-1/","publishdate":"2020-08-28T14:20:30-04:00","relpermalink":"/research/research-topic-1/","section":"research","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec eget ex augue. Sed nec quam dui. Integer ullamcorper turpis id fermentum venenatis. In dictum tellus eu sagittis maximus. Nulla facilisi. Curabitur sodales ligula nec leo pharetra rhoncus. Etiam enim ipsum, ultricies dictum libero sit amet, congue aliquet risus. Sed sed ipsum aliquam, luctus purus auctor, ultricies nisl. Fusce nec arcu id ex sodales egestas. Proin suscipit nunc diam, sit amet mattis dolor posuere eu. Fusce a laoreet diam. Pellentesque malesuada, nisl a interdum condimentum, lacus tortor dictum quam, nec varius sapien leo vel elit. Pellentesque porta sem ac nisi imperdiet mattis.\nFusce hendrerit quam urna, eu porta tortor maximus quis. Vestibulum sed sollicitudin nisi. Nulla sed nisl in mi tempus accumsan. Aenean iaculis nisi mauris. Cras quis sagittis nisi. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer eu magna nec nibh varius venenatis vitae sit amet est. Nam dapibus dui sit amet purus facilisis, vel facilisis nisl tristique. Vestibulum in velit vel lectus rutrum sagittis. Integer et iaculis ipsum, at luctus nibh. Donec sed justo at nunc facilisis lobortis eu laoreet enim. Maecenas et ante vitae nibh feugiat posuere ac vel ante. Aliquam quis nisl rutrum, pharetra felis et, ultricies est. Curabitur nec cursus diam, feugiat pharetra nisi. Phasellus eu velit sed mi luctus interdum at in est. Pellentesque ullamcorper neque in massa fermentum vehicula. Nulla massa eros, efficitur tempus blandit quis, venenatis non arcu. In hac habitasse platea dictumst. ","tags":[],"title":"Parallel Storage Systems","type":"research"},{"authors":[],"categories":[],"content":"","date":1598638830,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598638830,"objectID":"afed679e1bd9c1f374244663607d3171","permalink":"https://goodwillcomputinglab.github.io/research/research-topic-3/","publishdate":"2020-08-28T14:20:30-04:00","relpermalink":"/research/research-topic-3/","section":"research","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec eget ex augue. Sed nec quam dui. Integer ullamcorper turpis id fermentum venenatis. In dictum tellus eu sagittis maximus. Nulla facilisi. Curabitur sodales ligula nec leo pharetra rhoncus. Etiam enim ipsum, ultricies dictum libero sit amet, congue aliquet risus. Sed sed ipsum aliquam, luctus purus auctor, ultricies nisl. Fusce nec arcu id ex sodales egestas. Proin suscipit nunc diam, sit amet mattis dolor posuere eu. Fusce a laoreet diam. Pellentesque malesuada, nisl a interdum condimentum, lacus tortor dictum quam, nec varius sapien leo vel elit. Pellentesque porta sem ac nisi imperdiet mattis.\nFusce hendrerit quam urna, eu porta tortor maximus quis. Vestibulum sed sollicitudin nisi. Nulla sed nisl in mi tempus accumsan. Aenean iaculis nisi mauris. Cras quis sagittis nisi. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer eu magna nec nibh varius venenatis vitae sit amet est. Nam dapibus dui sit amet purus facilisis, vel facilisis nisl tristique. Vestibulum in velit vel lectus rutrum sagittis. Integer et iaculis ipsum, at luctus nibh. Donec sed justo at nunc facilisis lobortis eu laoreet enim. Maecenas et ante vitae nibh feugiat posuere ac vel ante. Aliquam quis nisl rutrum, pharetra felis et, ultricies est. Curabitur nec cursus diam, feugiat pharetra nisi. Phasellus eu velit sed mi luctus interdum at in est. Pellentesque ullamcorper neque in massa fermentum vehicula. Nulla massa eros, efficitur tempus blandit quis, venenatis non arcu. In hac habitasse platea dictumst. ","tags":[],"title":"Quantum Computing Systems","type":"research"},{"authors":[],"categories":[],"content":"","date":1598638830,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598638830,"objectID":"de6d16574abe14b3d260c24f52291b19","permalink":"https://goodwillcomputinglab.github.io/artifacts/post1/","publishdate":"2020-08-28T14:20:30-04:00","relpermalink":"/artifacts/post1/","section":"artifacts","summary":"New breakthrough from Northeastern Lab creates super machines that makes Quantum Computing available to everyone","tags":[],"title":"Quantum Computing Takes Over The World","type":"artifacts"},{"authors":[],"categories":[],"content":"","date":1598638830,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598638830,"objectID":"0345d9c91c7a3feb00304d88e94dd76c","permalink":"https://goodwillcomputinglab.github.io/news/post1/","publishdate":"2020-08-28T14:20:30-04:00","relpermalink":"/news/post1/","section":"news","summary":"New breakthrough from Northeastern Lab creates super machines that makes Quantum Computing available to everyone","tags":[],"title":"Quantum Computing Takes Over The World","type":"news"},{"authors":["Tirthak Patel","Devesh Tiwari"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"1b2624c4db4be50e3d460cdee1c83e0d","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confhpca-patel-t-20/","publishdate":"2020-07-24T21:05:21.258024Z","relpermalink":"/publication/dblp-confhpca-patel-t-20/","section":"publication","summary":"Large-scale data centers run latency-critical jobs with quality-of-service (QoS) requirements, and throughput-oriented background jobs, which need to achieve high perfor-mance. Previous works have proposed methods which cannot co-locate multiple latency-critical jobs with multiple back-grounds jobs while: (1) meeting the QoS requirements of all latency-critical jobs, and (2) maximizing the performance of the background jobs. This paper proposes CLITE, a Bayesian Optimization-based, multi-resource partitioning technique which achieves these goals.","tags":null,"title":"CLITE: Efficient and QoS-Aware Co-Location of Multiple Latency-Critical Jobs for Warehouse Scale Computers","type":"publication"},{"authors":["Heriberto A. Garcia","Trenton Couture","Amit Galor","Jessica M. Topple","Wei Huang","Devesh Tiwari","Purnima Ratilal"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"33078976b3050831601278ce6d7716a1","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-journalsremotesensing-garcia-cgthtr-20/","publishdate":"2020-07-24T21:05:21.251106Z","relpermalink":"/publication/dblp-journalsremotesensing-garcia-cgthtr-20/","section":"publication","summary":"A large variety of sound sources in the ocean, including biological, geophysical, and man-made, can be simultaneously monitored over instantaneous continental-shelf scale regions via the passive ocean acoustic waveguide remote sensing (POAWRS) technique by employing a large-aperture densely-populated coherent hydrophone array system. Millions of acoustic signals received on the POAWRS system per day can make it challenging to identify individual sound sources. An automated classification system is necessary to enable sound sources to be recognized. Here, the objectives are to (i) gather a large training and test data set of fin whale vocalization and other acoustic signal detections; (ii) build multiple fin whale vocalization classifiers, including a logistic regression, support vector machine (SVM), decision tree, convolutional neural network (CNN), and long short-term memory (LSTM) network; (iii) evaluate and compare performance of these classifiers using multiple metrics including accuracy, precision, recall and F1-score; and (iv) integrate one of the classifiers into the existing POAWRS array and signal processing software. The findings presented here will (1) provide an automatic classifier for near real-time fin whale vocalization detection and recognition, useful in marine mammal monitoring applications; and (2) lay the foundation for building an automatic classifier applied for near real-time detection and recognition of a wide variety of biological, geophysical, and man-made sound sources typically detected by the POAWRS system in the ocean","tags":null,"title":"Comparing Performances of Five Distinct Automatic Classifiers for Fin Whale Vocalizations in Beamformed Spectrograms of Coherent Hydrophone Array","type":"publication"},{"authors":["Tirthak Patel","Rohan Garg","Devesh Tiwari"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"80a2076b8cbfe74d8ce6b060e87308f1","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-conffast-patel-0-t-20/","publishdate":"2020-07-24T21:05:21.256023Z","relpermalink":"/publication/dblp-conffast-patel-0-t-20/","section":"publication","summary":"Large-scale parallel applications are highly data-intensive and perform terabytes of I/O routinely. Unfortunately, on a large-scale system where multiple applications run concurrently, I/O contention negatively affects system efficiency and causes unfair bandwidth allocation among applications. To address these challenges, this paper introduces GIFT, a principled dynamic approach to achieve fairness among competing applications and improve system efficiency.","tags":null,"title":"GIFT: A Coupon Based Throttle-and-Reward Mechanism for Fair and Efficient I/O Bandwidth Management on Parallel Storage Systems","type":"publication"},{"authors":["Sidi Lu","Bing Luo","Tirthak Patel","Yongtao Yao","Devesh Tiwari","Weisong Shi"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"60098fa9c2b78c5c5636efd735b10f33","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-conffast-lu-lpyts-20/","publishdate":"2020-07-24T21:05:21.257024Z","relpermalink":"/publication/dblp-conffast-lu-lpyts-20/","section":"publication","summary":"Disk drives are one of the most commonly replaced hardware components and continue to pose challenges for accurate failure prediction. In this work, we present analysis and findings from one of the largest disk failure prediction studies covering a total of 380,000 hard drives over a period of two months across 64 sites of a large leading data center operator. Our proposed machine learning based models predict disk failures with 0.95 F-measure and 0.95 Matthews correlation coefficient (MCC) for 10-days prediction horizon on average.","tags":null,"title":"Making Disk Failure Predictions SMARTer!","type":"publication"},{"authors":["Auroop Ganguly","Tanay Mehta","Tirthak Patel","Ravi Sundaram","Devesh Tiwari"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"0e628bda03988ac4d7783f1fe7c7853f","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-journalssnam-ganguly-mpst-20/","publishdate":"2020-07-24T21:05:21.255022Z","relpermalink":"/publication/dblp-journalssnam-ganguly-mpst-20/","section":"publication","summary":"We propose a new model for the study of resilience of coevolving multiplex scale-free networks. Our network model, called preferential interdependent networks, is a novel continuum over scale-free networks parameterized by their correlation ρ,0≤ρ≤1. Our failure and recovery model ties the propensity of a node, both to fail and to assist in recovery, to its importance. We show, analytically, that our network model can achieve any γ,2≤γ≤3 for the exponent of the power law of the degree distribution; this is superior to existing multiplex models and allows us better fidelity in representing real-world networks. Our failure and recovery model is also a departure from the much studied cascading error model based on the giant component; it allows for surviving important nodes to send assistance to the damaged nodes to enable their recovery. This better reflects the reality of recovery in man-made networks such as social networks and infrastructure networks. Our main finding, based on simulations, is that resilient preferential interdependent networks are those in which the layers are neither completely correlated (ρ=1) nor completely uncorrelated (ρ=0) but instead semi-correlated (ρ≈0.1−0.3). This finding is consistent with the real-world experience where complex man-made networks typically bounce back quickly from stress. In an attempt to explain our intriguing empirical discovery, we present an argument for why semi-correlated multiplex networks can be the most resilient. Our argument can be seen as an explanation of plausibility or as an incomplete mathematical proof subject to certain technical conjectures that we make explicit.","tags":null,"title":"Resilience and coevolution of preferential interdependent networks","type":"publication"},{"authors":["Tirthak Patel","Suren Byna","Glenn K. Lockwood","Nicholas J. Wright","Philip H. Carns","Robert B. Ross","Devesh Tiwari"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"177a533bc60c01d51c34551473652bd7","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-conffast-patel-blwcrt-20/","publishdate":"2020-07-24T21:05:21.255022Z","relpermalink":"/publication/dblp-conffast-patel-blwcrt-20/","section":"publication","summary":"Large-scale high-performance computing (HPC) applications running on supercomputers produce large amounts of data routinely and store it in files on multi-PB shared parallel storage systems. Unfortunately, storage community has a limited understanding of the access and reuse patterns of these files. This paper investigates the access and reuse patterns of I/O- intensive files on a production-scale supercomputer.","tags":null,"title":"Uncovering Access, Reuse, and Sharing Characteristics of I/O-Intensive Files on Large-Scale Production HPC Systems","type":"publication"},{"authors":["Tirthak Patel","Adam Wagenhäuser","Christopher Eibel","Timo Hönig","Thomas Zeiser","Devesh Tiwari"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"b39799dd95312735f73444dce2f954ba","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confipps-patel-wehzt-20/","publishdate":"2020-07-24T21:05:21.259026Z","relpermalink":"/publication/dblp-confipps-patel-wehzt-20/","section":"publication","summary":"As we approach exascale computing, large-scale HPC systems are becoming increasingly power-constrained, requiring them to run HPC workloads in an energy-efficient manner. The first step toward achieving this goal is to better understand, analyze, and quantify the power consumption characteristics of HPC jobs. However, there is a lack of understanding of the power consumption characteristics of HPC jobs which run on production HPC systems. Such characterization is required to guide the design of the next generation of power-aware resource management. To the best of our knowledge, we are the first study to open-source the data and analysis of power-consumption characteristics of HPC jobs and users from two medium-scale production HPC clusters.","tags":null,"title":"What does Power Consumption Behavior of HPC Jobs Reveal? : Demystifying, Quantifying, and Predicting Power Consumption Characteristics","type":"publication"},{"authors":["Hyogi Sim","Geoffroy Vallée","Youngjae Kim","Sudharshan S. Vazhkudai","Devesh Tiwari","Ali Raza Butt"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"b23cd1d77da45237e41cb226702dd065","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-journalstpds-sim-vkvtb-19/","publishdate":"2020-07-24T21:05:21.259026Z","relpermalink":"/publication/dblp-journalstpds-sim-vkvtb-19/","section":"publication","summary":"The need for novel data analysis is urgent in the face of a data deluge from modern applications. Traditional approaches to data analysis incur significant data movement costs, moving data back and forth between the storage system and the processor. Emerging Active Flash devices enable processing on the flash, where the data already resides. An array of such Active Flash devices allows us to revisit how analysis workflows interact with storage systems. By seamlessly blending together the flash storage and data analysis, we create an analysis workflow-aware storage system, AnalyzeThis. Our guiding principle is that analysis-awareness be deeply ingrained in each and every layer of the storage, elevating data analyses as first-class citizens, and transforming AnalyzeThis into a potent analytics-aware appliance. To evaluate the AnalyzeThis system, we have adopted both emulation and simulation approaches. In particular, we have evaluated AnalyzeThis by implementing the AnalyzeThis storage system on top of the Active Flash Array's emulation platform. We have also implemented an event-driven AnalyzeThis simulator, called AnalyzeThisSim, which allows us to address the limitations of the emulation platform, e.g., performance impact of using multi-core SSDs. The results from our emulation and simulation platforms indicate that AnalyzeThis is a viable approach for expediting workflow execution and minimizing data movement.","tags":null,"title":"An Analysis Workflow-Aware Storage System for Multi-Core Active Flash Arrays","type":"publication"},{"authors":["Song Huang","Shuwen Liang","Song Fu","Weisong Shi","Devesh Tiwari","Hsing-bung Chen"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"91ec01a710d38f62c6f14afd2b04d6b1","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-conficac-huang-lfstc-19/","publishdate":"2020-07-24T21:05:21.26403Z","relpermalink":"/publication/dblp-conficac-huang-lfstc-19/","section":"publication","summary":"The booming of cloud computing, online services and big data applications have resulted in dramatic expansion of storage systems. Meanwhile, disk drives are reported to be the most commonly replaced hardware component. Disk failures cause service downtime and even data loss, costing enterprises multi-trillion dollars per year. Existing disk failure management approaches are mostly reactive and incur high overheads. To overcome these problems, in this paper, we present a proactive, cost-effective solution to managing large-scale production storage systems. We aim to uncover the entire process in which disk's health deteriorates and forecast when disk drives will fail in the future. Due to a common lack of diagnostic information of disk failures, we rely on the Self-Monitoring, Analysis and Reporting Technology (SMART) data and explore statistical analysis techniques to identify the start of disk degradation. We then model the disk degradation processes as functions of SMART attributes, which eliminates the dependency on time and thus I/O workload. Experimental results from over 23,000 enterprise-class disk drives in a production data center show that our derived models can accurately quantify the degradation of disk health, which enables us to proactively protect data against disk failures. We also investigate several types of disk failures and propose remediation mechanisms to prolong disk lifetime.","tags":null,"title":"Characterizing Disk Health Degradation and Proactively Protecting Against Disk Failures for Reliable Storage Systems","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://goodwillcomputinglab.github.io/contact/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"Hello!","tags":null,"title":"Contact Us","type":"widget_page"},{"authors":["Gourav Rattihalli","Madhusudhan Govindaraju","Hui Lu","Devesh Tiwari"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"4aae76be71a0d078957f5c646d4dedb7","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confiee-ecloud-rattihalli-glt-19/","publishdate":"2020-07-24T21:05:21.260026Z","relpermalink":"/publication/dblp-confiee-ecloud-rattihalli-glt-19/","section":"publication","summary":"Cloud platforms typically require users to provide resource requirements for applications so that resource managers can schedule containers with adequate allocations. However, the requirements for container resources often depend on numerous factors such as application input parameters, optimization flags, input files, and attributes that are specified for each run. So, it is complex for users to estimate the resource requirements for a given container accurately, leading to resource over-estimation that negatively affects overall utilization. We have designed a Resource Utilization Based Autoscaling System (RUBAS) that can dynamically adjust the allocation of containers running in a Kubernetes cluster. RUBAS improves upon the Kubernetes Vertical Pod Autoscaler (VPA) system non-disruptively by incorporating container migration. Our experiments use multiple scientific benchmarks. We analyze the allocation pattern of RUBAS with Kubernetes VPA. We compare the performance of container migration for in-place and remote node migration and we evaluate the overhead in RUBAS. Our results show that compared to Kubernetes VPA, RUBAS improves the CPU and memory utilization of the cluster by 10% and reduces the runtime by 15% with an overhead for each application ranging from 5% to 20%.","tags":null,"title":"Exploring Potential for Non-Disruptive Vertical Auto Scaling and Resource Estimation in Kubernetes","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"4d4886567bb6823fa039ec45fd21b24a","permalink":"https://goodwillcomputinglab.github.io/outreach/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/outreach/","section":"","summary":"Outreach work of the lab","tags":null,"title":"Goodwill Outreach","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://goodwillcomputinglab.github.io/people/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"Hello!","tags":null,"title":"Landing Page","type":"widget_page"},{"authors":["Fritz G. Previlon","Charu Kalra","Devesh Tiwari","David R. Kaeli"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"d2e98bb9b980a236d6aae338da435b2f","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confdate-previlon-ktk-19/","publishdate":"2020-07-24T21:05:21.263029Z","relpermalink":"/publication/dblp-confdate-previlon-ktk-19/","section":"publication","summary":"Reliability has become a first-class design objective for GPU devices due to increasing soft-error rate. To assess the reliability of GPU programs, researchers rely on software fault-injection methods. Unfortunately, software fault-injection process is prohibitively expensive, requiring multiple days to complete a statistically sound fault-injection campaign. Therefore, to address this challenge, this paper proposes a novel fault-injection method, PCFI, that reduces the number of fault injections by exploiting the predictability in fault-injection outcome based on the program counter of the soft-error affected instruction. Evaluation on a variety of GPU programs covering a wide range of application domains shows that PCFI reduces the time to complete fault-injection campaigns by 22% on average, without sacrificing accuracy.","tags":null,"title":"PCFI: Program Counter Guided Fault Injection for Accelerating GPU Reliability Assessment","type":"publication"},{"authors":["Tirthak Patel","Devesh Tiwari"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"bc51dea2095d4bc4f2ffe0c727aad8dd","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confhpdc-patel-t-19/","publishdate":"2020-07-24T21:05:21.263029Z","relpermalink":"/publication/dblp-confhpdc-patel-t-19/","section":"publication","summary":"Large-scale computing systems are becoming increasingly more power-constrained, but these systems employ hardware over- provisioning to achieve higher system throughput because applications often do not consume the peak power capacity of nodes. Unfortunately, focusing on system throughput alone can lead to severe unfairness among multiple concurrently-running applications. This paper introduces PERQ, a new feedback-based principled approach to improve system throughput while achieving fairness among concurrent applications.","tags":null,"title":"PERQ: Fair and Efficient Power Management of Power-Constrained Large-Scale Computing Systems","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"f6148aeae41da782d92294ad6c53cf04","permalink":"https://goodwillcomputinglab.github.io/recent-publication/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/recent-publication/","section":"","summary":"Recent Publications","tags":null,"title":"Recent Publications","type":"widget_page"},{"authors":["Tirthak Patel","Suren Byna","Glenn K. Lockwood","Devesh Tiwari"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"f198b9a054e7e82f8e793658941a0c9e","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confsc-patel-blt-19/","publishdate":"2020-07-24T21:05:21.265031Z","relpermalink":"/publication/dblp-confsc-patel-blt-19/","section":"publication","summary":"Large-scale applications typically spend a large fraction of their execution time performing I/O to a parallel storage system. However, with rapid progress in compute and storage system stack of large-scale systems, it is critical to investigate and update our understanding of the I/O behavior of large-scale applications. Toward that end, in this work, we monitor, collect and analyze a year worth of storage system data from a large-scale production parallel storage system. We perform temporal, spatial and correlative analysis of the system and uncover surprising patterns which defy existing assumptions and have important implications for future systems.","tags":null,"title":"Revisiting I/O behavior in large-scale storage systems: the expected and the unexpected","type":"publication"},{"authors":["Gourav Rattihalli","Madhusudhan Govindaraju","Devesh Tiwari"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"2ea933601f0ff719c2ecbfc3a5063faa","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confccgrid-rattihalli-gt-19/","publishdate":"2020-07-24T21:05:21.261027Z","relpermalink":"/publication/dblp-confccgrid-rattihalli-gt-19/","section":"publication","summary":"Academic cloud infrastructures require users to specify an estimate of their resource requirements. The resource usage for applications often depends on the input file sizes, parameters, optimization flags, and attributes, specified for each run. Incorrect estimation can result in low resource utilization of the entire infrastructure and long wait times for jobs in the queue. We have designed a Resource Utilization based Migration (RUMIG) system to address the resource estimation problem. We present the overall architecture of the two-stage elastic cluster design, the Apache Mesos-specific container migration system, and analyze the performance for several scientific workloads on three different cloud/cluster environments. In this paper we (b) present a design and implementation for container migration in a Mesos environment, (c) evaluate the effect of right-sizing and cluster elasticity on overall performance, (d) analyze different profiling intervals to determine the best fit, (e) determine the overhead of our profiling mechanism. Compared to the default use of Apache Mesos, in the best cases, RUMIG provides a gain of 65% in runtime (local cluster), 51% in CPU utilization in the Chameleon cloud, and 27% in memory utilization in the Jetstream cloud.","tags":null,"title":"Towards Enabling Dynamic Resource Estimation and Correction for Improving Utilization in an Apache Mesos Cloud Environment","type":"publication"},{"authors":["Gourav Rattihalli","Pankaj Saha","Madhusudhan Govindaraju","Devesh Tiwari"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"950d3e488e98ed9f81af38199b452f38","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-journalscorrabs-1905-09166/","publishdate":"2020-07-24T21:05:21.266031Z","relpermalink":"/publication/dblp-journalscorrabs-1905-09166/","section":"publication","summary":"As resource estimation for jobs is difficult, users often overestimate their requirements. Both commercial clouds and academic campus clusters suffer from low resource utilization and long wait times as the resource estimates for jobs, provided by users, is inaccurate. We present an approach to statistically estimate the actual resource requirement of a job in a Little cluster before the run in a Big cluster. The initial estimation on the little cluster gives us a view of how much actual resources a job requires. This initial estimate allows us to accurately allocate resources for the pending jobs in the queue and thereby improve throughput and resource utilization. In our experiments, we determined resource utilization estimates with an average accuracy of 90% for memory and 94% for CPU, while we make better utilization of memory by an average of 22% and CPU by 53%, compared to the default job submission methods on Apache Aurora and Apache Mesos. ","tags":null,"title":"Two stage cluster for resource optimization with Apache Mesos","type":"publication"},{"authors":["Janki Bhimani","Tirthak Patel","Ningfang Mi","Devesh Tiwari"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"7ec63da77ebe17bb23285da1b692be74","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confdac-bhimani-pmt-19/","publishdate":"2020-07-24T21:05:21.262028Z","relpermalink":"/publication/dblp-confdac-bhimani-pmt-19/","section":"publication","summary":"Vibration generated in modern computing environments such as autonomous vehicles, edge computing infrastructure, and data center systems is an increasing concern. In this paper, we systematically measure, quantify and characterize the impact of vibration on the performance of SSD devices. Our experiments and analysis uncover that exposure to both short-term and long-term vibration, even within the vendor-specified limits, can significantly affect SSD I/O performance and reliability.","tags":null,"title":"What does Vibration do to Your SSD?","type":"publication"},{"authors":["Kun Tang","Xubin He","Saurabh Gupta","Sudharshan S. Vazhkudai","Devesh Tiwari"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"b005aaa86ab98257f8f9eb17ccc832b3","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-conficccn-tang-hgvt-18/","publishdate":"2020-07-24T21:05:21.271036Z","relpermalink":"/publication/dblp-conficccn-tang-hgvt-18/","section":"publication","summary":"In high-performance computing (HPC) workflows, data analytics is typically utilized to gain insights from scientific simulations. Approaching the era of exascale, online analysis is gaining popularity due to the savings of I/O to persistent storage. As computing capability keeps growing, power consumption is becoming critical to HPC facilities. Enforcing power limits is emerging as a practical trend for power-constrained HPC facilities. However, it remains unclear how to choose the appropriate power limits for various HPC workflows and how to distribute the power limit of a workflow between simulation and analysis. In addition, given a power limit, it is unclear what the optimal scales and power capping levels are for various workflows, especially when taking reliability into account. In order to resolve these issues in power-constrained HPC, in this paper, we propose a reliability-aware model to determine the aforementioned platform configurations for HPC workflows. We also validate our model and present model-driven studies for a wide range of real-system scenarios. Our study reveals interesting insights about how platform configuration affects the performance and energy efficiency of HPC workflows under power constraints.","tags":null,"title":"Exploring the Optimal Platform Configuration for Power-Constrained HPC Workflows","type":"publication"},{"authors":["Bin Nie","Ji Xue","Saurabh Gupta","Tirthak Patel","Christian Engelmann","Evgenia Smirni","Devesh Tiwari"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"941802deb3419ed35080f502ac50e001","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confdsn-nie-xgpest-18/","publishdate":"2020-07-24T21:05:21.269034Z","relpermalink":"/publication/dblp-confdsn-nie-xgpest-18/","section":"publication","summary":"GPUs are widely deployed on large-scale HPC systems to provide powerful computational capability for scientific applications from various domains. As those applications are normally long-running, investigating the characteristics of GPU errors becomes imperative for reliability. In this paper, we first study the system conditions that trigger GPU errors using six-month trace data collected from a large-scale, operational HPC system. Then, we use machine learning to predict the occurrence of GPU errors, by taking advantage of temporal and spatial dependencies of the trace data. The resulting machine learning prediction framework is robust and accurate under different workloads.","tags":null,"title":"Machine Learning Models for GPU Error Prediction in a Large Scale HPC System","type":"publication"},{"authors":["Shuwen Liang","Zhi Qiao","Jacob Hochstetler","Song Huang","Song Fu","Weisong Shi","Devesh Tiwari","Hsing-Bung Chen","Bradley W. Settlemyer","David Richard Montoya"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"d91be55c18f8fb422cb8e9766760f83d","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confbigdataconf-liang-qhhfstcsm-18/","publishdate":"2020-07-24T21:05:21.268033Z","relpermalink":"/publication/dblp-confbigdataconf-liang-qhhfstcsm-18/","section":"publication","summary":"In recent years, NAND flash-based solid state drives (SSD) have been widely used in datacenters due to their better performance compared with the traditional hard disk drives. However, little is known about the reliability characteristics of SSDs in production systems. Existing works study the statistical distributions of SSD failures in the field. However, they do not go deep into SSD drives and investigate the unique error types and health dynamics that distinguish SSDs from hard disk drives. In this paper, we explore the SSD-specific SMART (Self-Monitoring, Analysis, and Reporting Technology) attributes to conduct an in-depth analysis of SSD reliability in a production environment. Data is collected from a scalable production system having several physical locations. Our dataset contains over a million records with more than twenty attributes. We leverage machine learning technologies, specifically data clustering and correlation analysis methods, to discover groups of SSDs which have different health status and relations among SSD-specific SMART attributes. Our results show that 1) Media wear affects the reliability of SSDs more than any other factors, and 2) SSDs transit from one health group to another which infers the reliability degradation of those drives. To the best of our knowledge, this is the first study that investigates SSD-specific SMART data to characterize SSD reliability in a production environment.","tags":null,"title":"Reliability Characterization of Solid State Drives in a Scalable Production Datacenter","type":"publication"},{"authors":["Auroop Ganguly","Tanbay Mehta","Ravi Sundaram","Devesh Tiwari"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"723acc97638e87a61408c2393e21c4ee","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confasunam-ganguly-mst-18/","publishdate":"2020-07-24T21:05:21.267033Z","relpermalink":"/publication/dblp-confasunam-ganguly-mst-18/","section":"publication","summary":"We propose a new model for the study of resilience of coevolving multiplex scale-free networks. Our network model, called preferential interdependent networks, is a novel continuum over scale-free networks parameterized by their correlation p, 0 ≤ p ≤1. Our failure and recovery model ties the propensity of a node, both to fail and to assist in recovery, to its importance. We show, analytically, that our network model can achieve any γ, 2 ≤ γ ≤ 3 for the exponent of the power law of the degree distribution; this is superior to existing multiplex models and allows us better fidelity in representing real-world networks. Our failure and recovery model is also a departure from the much studied cascading error model based on the giant component; it allows for surviving important nodes to send assistance to the damaged nodes to enable their recovery. This better reflects the reality of recovery in man-made networks such as social networks and infrastructure networks. Our main finding, based on simulations, is that resilient preferential interdependent networks are those in which the layers are neither completely correlated (p = 1) nor completely uncorrelated (p= 0) but instead semi-correlated (p ≈ 0.1 - 0.3). This finding is consistent with the real-world experience where complex man-made networks typically bounce back quickly from stress. In an attempt to explain our intriguing empirical discovery we present an argument for why semi-correlated multiplex networks can be the most resilient. Our argument can be seen as an explanation of plausibility or as an incomplete mathematical proof subject to certain technical conjectures that we make explicit.","tags":null,"title":"Resilience and the Coevolution of Interdependent Multiplex Networks","type":"publication"},{"authors":["Rohan Garg","Tirthak Patel","Gene Cooperman","Devesh Tiwari"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"de98c5e5605aeddfadb7358d6770f231","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confdsn-garg-pct-18/","publishdate":"2020-07-24T21:05:21.268033Z","relpermalink":"/publication/dblp-confdsn-garg-pct-18/","section":"publication","summary":"Large-scale applications rely on resilience mechanisms such as checkpoint-restart to make forward progress in the presence of failures. Unfortunately, this incurs huge I/O overhead and impedes productivity. To mitigate this challenge, this paper introduces a new technique, Shiraz, which demonstrates how to exploit differences in the checkpointing overhead among applications and knowledge of temporal characteristics of failures to improve both the overall system throughput and performance of individual applications.","tags":null,"title":"Shiraz: Exploiting System Reliability and Application Resilience Characteristics to Improve Large Scale System Throughput","type":"publication"},{"authors":["Mohit Kumar","Saurabh Gupta","Tirthak Patel","Michael Wilder","Weisong Shi","Song Fu","Christian Engelmann","Devesh Tiwari"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"2a2518259e38db4be2225ec7b1784ead","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confdsn-kumar-gpwsfet-18/","publishdate":"2020-07-24T21:05:21.270036Z","relpermalink":"/publication/dblp-confdsn-kumar-gpwsfet-18/","section":"publication","summary":"Today's High Performance Computing (HPC) systems are capable of delivering performance in the order of petaflops due to the fast computing devices, network interconnect, and back-end storage systems. In particular, interconnect resilience and congestion resolution methods have a major impact on the overall interconnect and application performance. This is especially true for scientific applications running multiple processes on different compute nodes as they rely on fast network messages to communicate and synchronize frequently. Unfortunately, the HPC community lacks state-of-practice experience reports that detail how different interconnect errors and congestion events occur on large-scale HPC systems. Therefore, in this paper, we process and analyze interconnect data of the Titan supercomputer to develop a thorough understanding of interconnects faults, errors and congestion events. We also study the interaction between interconnect, errors, network congestion and application characteristics.","tags":null,"title":"Understanding and Analyzing Interconnect Errors and Network Congestion on a Large Scale HPC System","type":"publication"},{"authors":["Bin Nie","Ji Xue","Saurabh Gupta","Christian Engelmann","Evgenia Smirni","Devesh Tiwari"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"63e0bf67f25a608bc5f9f13ff5949e99","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confmascots-nie-xgest-17/","publishdate":"2020-07-24T21:05:21.274039Z","relpermalink":"/publication/dblp-confmascots-nie-xgest-17/","section":"publication","summary":"GPUs have become part of the mainstream high performance computing facilities that increasingly require more computational power to simulate physical phenomena quickly and accurately. However, GPU nodes also consume significantly more power than traditional CPU nodes, and high power consumption introduces new system operation challenges, including increased temperature, power/cooling cost, and lower system reliability. This paper explores how power consumption and temperature characteristics affect reliability, provides insights into what are the implications of such understanding, and how to exploit these insights toward predicting GPU errors using neural networks.","tags":null,"title":"Characterizing Temperature, Power, and Soft-Error Behaviors in Data Center Systems: Insights, Challenges, and Opportunities","type":"publication"},{"authors":["Fritz G. Previlon","Babatunde Egbantan","Devesh Tiwari","Paolo Rech","David R. Kaeli"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"6207ba2ccd117083b7c0feeca58f8e29","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confmwscas-previlon-etrk-17/","publishdate":"2020-07-24T21:05:21.276041Z","relpermalink":"/publication/dblp-confmwscas-previlon-etrk-17/","section":"publication","summary":"Transient faults continue to be a critical concern in a range of computing domains including: High-Performance Computing (HPC), scientific computing, and the automotive industry. While radiation-induced faults have been well studied and understood in microprocessors, their impact on computations on Graphic Processing Units (GPU) has received less attention. GPUs are now being used in a large number of HPC and automotive markets. Mitigating the effects of transient faults requires a thorough understanding of the interaction between applications, system software, and the underlying hardware. Developing this understanding is quite challenging mainly due to our limited ability to capture and study cross-layer reliability interactions. In this paper, we consider the combination of neutron beam testing experiments with architectural fault injection experiments to gain a deeper understanding of the relationship between the vulnerability of GPUs and the underlying workload characteristics of applications targeted for GPU devices.","tags":null,"title":"Combining architectural fault-injection and neutron beam testing approaches toward better understanding of GPU soft-error resilience","type":"publication"},{"authors":["Qingrui Liu","Changhee Jung","Dongyoon Lee","Devesh Tiwari"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"3afcf2b17939f788648d30dee60aba16","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-journalstecs-liu-jlt-17/","publishdate":"2020-07-24T21:05:21.272037Z","relpermalink":"/publication/dblp-journalstecs-liu-jlt-17/","section":"publication","summary":"This article presents Clover, a compiler-directed soft error detection and recovery scheme for lightweight soft error resilience. The compiler carefully generates soft-error-tolerant code based on idempotent processing without explicit checkpoints. During program execution, Clover relies on a small number of acoustic wave detectors deployed in the processor to identify soft errors by sensing the wave made by a particle strike. To cope with DUEs (detected unrecoverable errors) caused by the sensing latency of error detection, Clover leverages a novel selective instruction duplication technique called tail-DMR (dual modular redundancy) that provides a region-level error containment. Once a soft error is detected by either the sensors or the tail-DMR, Clover takes care of the error as in the case of exception handling. To recover from the error, Clover simply redirects program control to the beginning of the code region where the error is detected. The experimental results demonstrate that the average runtime overhead is only 26%, which is a 75% reduction compared to that of the state-of-the-art soft error resilience technique. In addition, this article evaluates an alternative technique called tail-wait, comparing it to Clover. According to the evaluation with the different processor configurations and the various error detection latencies, Clover turns out to be a superior technique, achieving 1.06 to 3.49 × speedup over the tail-wait.","tags":null,"title":"Compiler-Directed Soft Error Detection and Recovery to Avoid DUE and SDC via Tail-DMR","type":"publication"},{"authors":["Kun Tang","Devesh Tiwari","Saurabh Gupta","Sudharshan S. Vazhkudai","Xubin He"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"85041d00fe35780cefca06607abb1f11","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confcluster-tang-tgvh-17/","publishdate":"2020-07-24T21:05:21.273038Z","relpermalink":"/publication/dblp-confcluster-tang-tgvh-17/","section":"publication","summary":"In high-performance computing (HPC), end-to-end workflows are typically utilized to gain insights from scientific simulations. An end-to-end workflow consists of scientific simulation and data analysis, and can be executed in-situ, in-transit, and offline. Existing studies on end-to-end workflows have largely focused on the high-performance execution approaches. However, the emerging heterogeneous architectures and energy concerns lead to the rethinking of workflow execution approaches. As a guide to the rethinking, this paper evaluates how to run end-to-end HPC workflows efficiently in terms of performance, energy, and error resilience. The evaluation covers emerging heterogeneous processor architectures, processor power capping techniques, and heterogeneous-reliability memory.","tags":null,"title":"Effective Running of End-to-End HPC Workflows on Emerging Heterogeneous Architectures","type":"publication"},{"authors":["Saurabh Gupta","Tirthak Patel","Christian Engelmann","Devesh Tiwari"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"682bfc8cfaca06ff62d294e69707c713","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confsc-gupta-pet-17/","publishdate":"2020-07-24T21:05:21.276041Z","relpermalink":"/publication/dblp-confsc-gupta-pet-17/","section":"publication","summary":"Resilience is one of the key challenges in maintaining high efficiency of future extreme scale supercomputers. Researchers and system practitioners rely on field-data studies to understand reliability characteristics and plan for future HPC systems. In this work, we compare and contrast the reliability characteristics of multiple large-scale HPC production systems. Our study covers more than one billion compute node hours across five different systems over a period of 8 years. We confirm previous findings which continue to be valid, discover new findings, and discuss their implications.","tags":null,"title":"Failures in large scale systems: long-term measurement, analysis, and implications","type":"publication"},{"authors":["Sudharshan S. Vazhkudai","Ross G. Miller","Devesh Tiwari","Christopher Zimmer","Feiyi Wang","Sarp Oral","Raghul Gunasekaran","Deryl Steinert"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"2d7dfef7be377372c0fab2db73d2e6ec","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confsc-vazhkudai-mtzwog-17/","publishdate":"2020-07-24T21:05:21.277042Z","relpermalink":"/publication/dblp-confsc-vazhkudai-mtzwog-17/","section":"publication","summary":"In this paper, we describe the GUIDE framework used to collect, federate, and analyze log data from the Oak Ridge Leadership Computing Facility (OLCF), and how we use that data to derive insights into facility operations. We collect system logs and extract monitoring data at every level of the various OLCF subsystems, and have developed a suite of pre-processing tools to make the raw data consumable. The cleansed logs are then ingested and federated into a central, scalable data warehouse, Splunk, that offers storage, indexing, querying, and visualization capabilities. We have further developed and deployed a set of tools to analyze these multiple disparate log streams in concert and derive operational insights. We describe our experience from developing and deploying the GUIDE infrastructure, and deriving valuable insights on the various subsystems, based on two years of operations in the production OLCF environment.","tags":null,"title":"GUIDE: a scalable information directory service to collect, federate, and analyze logs for operational insights into a leadership HPC facility","type":"publication"},{"authors":["Jaimie Kelley","Christopher Stewart","Nathaniel Morris","Devesh Tiwari","Yuxiong He","Sameh Elnikety"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"74676617973f139fdb92a728f6d61b34","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-journalstompecs-kelley-smthe-17/","publishdate":"2020-07-24T21:05:21.272037Z","relpermalink":"/publication/dblp-journalstompecs-kelley-smthe-17/","section":"publication","summary":"Online data-intensive (OLDI) services use anytime algorithms to compute over large amounts of data and respond quickly. Interactive response times are a priority, so OLDI services parallelize query execution across distributed software components and return best effort answers based on the data so far processed. Omitted data from slow components could lead to better answers, but tracing online how much better the answers could be is difficult. We propose Ubora, a design approach to measure the effect of slow-running components on the quality of answers. Ubora randomly samples online queries and executes them a second time. The first online execution omits data from slow components and provides interactive answers. The second execution uses mature results from intermediate components completed after the online execution finishes. Ubora uses memoization to speed up mature executions by replaying network messages exchanged between components. Our systems-level implementation works for a wide range of services, including Hadoop/Yarn, Apache Lucene, the EasyRec Recommendation Engine, and the OpenEphyra question-answering system. Ubora computes answer quality with more mature executions per second than competing approaches that do not use memoization. With Ubora, we show that answer quality is effective at guiding online admission control. While achieving the same answer quality on high-priority queries, our adaptive controller had 55% higher peak throughput on low-priority queries than a competing controller guided by the rate of timeouts.","tags":null,"title":"Obtaining and Managing Answer Quality for Online Data-Intensive Services","type":"publication"},{"authors":["Kun Tang","Ping Huang","Xubin He","Tao Lu","Sudharshan S. Vazhkudai","Devesh Tiwari"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"acf3b50de971717ece9d406724b64cb3","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confmascots-tang-hhlvt-17/","publishdate":"2020-07-24T21:05:21.27504Z","relpermalink":"/publication/dblp-confmascots-tang-hhlvt-17/","section":"publication","summary":"HPC (high-performance computing) applications usually show bursty I/O behaviors. In order to expedite the applications, permanent storage systems are usually provisioned to serve such I/O bursts. Approaching the era of exascale computing, non-volatile RAM is introduced as burst buffers, to absorb the bursty bulk data and relax the I/O provisioning requirement of the permanent storage systems. However, without judiciously draining the burst buffers, I/O bursts are passed down to the underlying storage systems, which causes severe I/O contention issues.In order to minimize the I/O provisioning requirement and resolve the issues caused by I/O bursts, we propose a proactive draining scheme to manage the draining process of distributed node-local burst buffers. In addition, we develop an I/O provisioning model to predict the minimized I/O provisioning requirement for permanent storage systems. Evaluation results show that applying the proactive draining scheme largely relaxes the I/O provisioning requirement while preserving the I/O performance of underlying storage systems.","tags":null,"title":"Toward Managing HPC Burst Buffers Effectively: Draining Strategy to Regulate Bursty I/O Behavior","type":"publication"},{"authors":["Bin Nie","Devesh Tiwari","Saurabh Gupta","Evgenia Smirni","James H. Rogers"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"af975188a1dbc4ef6c19c26c519192d1","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confhpca-nie-tgsr-16/","publishdate":"2020-07-24T21:05:21.280045Z","relpermalink":"/publication/dblp-confhpca-nie-tgsr-16/","section":"publication","summary":"Parallelism provided by the GPU architecture has enabled domain scientists to simulate physical phenomena at a much faster rate and finer granularity than what was previously possible by CPU-based large-scale clusters. Architecture researchers have been investigating reliability characteristics of GPUs and innovating techniques to increase the reliability of these emerging computing devices. Such efforts are often guided by technology projections and simplistic scientific kernels, and performed using architectural simulators and modeling tools. Lack of large-scale field data impedes the effectiveness of such efforts. This study attempts to bridge this gap by presenting a large-scale field data analysis of GPU reliability. We characterize and quantify different kinds of soft-errors on the Titan supercomputer's GPU nodes. Our study uncovers several interesting and previously unknown insights about the characteristics and impact of soft-errors.","tags":null,"title":"A large-scale study of soft-errors on GPUs in the field","type":"publication"},{"authors":["Jaimie Kelley","Christopher Stewart","Devesh Tiwari","Saurabh Gupta"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"88d7c51fd945dbf4794540af4955f5e2","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-conficac-kelley-stg-16/","publishdate":"2020-07-24T21:05:21.280045Z","relpermalink":"/publication/dblp-conficac-kelley-stg-16/","section":"publication","summary":"State of the art schedulers use workload profiles to help determine which resources to allocate. Traditionally, threads execute on every available core, but increasingly, too much power is consumed by using every core. Because peak power can occur at any point in time during the workload, workloads are commonly profiled to completion multiple times in an offline architecture. In practice, this process is too time consuming for online profiling and alternate approaches are used, such as profiling for k% of the workload or predicting peak power from similar workloads. We studied the effectiveness of these methods for core scaling. Core scaling is a technique which executes threads on a subset of available cores, allowing unused cores to enter low-power operating modes. Schedulers can use core scaling to reduce peak power, but must have an accurate profile across potential settings for number of active cores in order to know when to make this decision. We devised an accurate, fast and adaptive approach to profile peak power under core scaling. Our approach uses short profiling runs to collect instantaneous power traces for a workload under each core scaling setting. The duration of profiling varies for each power trace and depends on the desired accuracy. Compared to k% profiling of peak power, our approach reduced the profiling duration by up to 93% while keeping accuracy within 3%.","tags":null,"title":"Adaptive Power Profiling for Many-Core HPC Architectures","type":"publication"},{"authors":["Shinan Wang","Bing Luo","Weisong Shi","Devesh Tiwari"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"3d0bda86f256aef0e1b566d8406e886e","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-journalsjpdc-wang-lst-16/","publishdate":"2020-07-24T21:05:21.278043Z","relpermalink":"/publication/dblp-journalsjpdc-wang-lst-16/","section":"publication","summary":"Modern computer systems are designed to balance performance and energy consumption. Several run-time factors, such as concurrency levels, thread mapping strategies, and dynamic voltage and frequency scaling (DVFS) should be considered in order to achieve optimal energy efficiency for a workload. Selecting appropriate run-time factors, however, is one of the most challenging tasks because the run-time factors are architecture-specific and workload-specific. While most existing works concentrate on either static analysis of the workload or run-time prediction results, in this paper, we present a hybrid two-step method that utilizes concurrency levels and DVFS settings to achieve the energy efficiency configuration for a workload. The experimental results based on a Xeon E5620 server with NPB and PARSEC benchmark suites show that the model is able to predict the energy efficient configuration accurately. On average, an additional EDP (Energy Delay Product) saving is obtained by using run-time DVFS for the entire system. An off-line optimal solution is used to compare with the proposed scheme. The experimental results show that the average extra EDP saved by the optimal solution is within 5% on selective parallel benchmarks","tags":null,"title":"Application configuration selection for energy-efficient execution on multicore systems","type":"publication"},{"authors":["Qingrui Liu","Changhee Jung","Dongyoon Lee","Devesh Tiwari"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"aeda3e399098df810b97537bc9126297","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confsc-liu-jlt-16/","publishdate":"2020-07-24T21:05:21.283048Z","relpermalink":"/publication/dblp-confsc-liu-jlt-16/","section":"publication","summary":"This paper presents Bolt, a compiler-directed soft error recovery scheme, that provides fine-grained and guaranteed recovery without excessive performance and hardware overhead. To get rid of expensive hardware support, the compiler protects the architectural inputs during their entire liveness period by safely checkpointing the last updated value in idempotent regions. To minimize the performance overhead, Bolt leverages a novel compiler analysis that eliminates those checkpoints whose value can be reconstructed by other checkpointed values without compromising the recovery guarantee. As a result, Bolt incurs only 4.7% performance overhead on average which is 57% reduction compared to the state-of-the-art scheme that requires expensive hardware support for the same recovery guarantee as Bolt.","tags":null,"title":"Compiler-directed lightweight checkpointing for fine-grained guaranteed soft error recovery","type":"publication"},{"authors":["Anshu Dubey","Hajime Fujita","Daniel T. Graves","Andrew A. Chien","Devesh Tiwari"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"9a2fd3a19c48a9fc05bb3641dbce0c71","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confsc-dubey-fgct-16/","publishdate":"2020-07-24T21:05:21.284048Z","relpermalink":"/publication/dblp-confsc-dubey-fgct-16/","section":"publication","summary":"Supercomputing platforms are expected to have larger failure rates in the future because of scaling and power concerns. The memory and performance impact may vary with error types and failure modes. Therefore, localized recovery schemes will be important for scientific computations, including failure modes where application intervention is suitable for recovery. We present a resiliency methodology for applications using structured adaptive mesh refinement, where failure modes map to granularities within the application for detection and correction. This approach also enables parameterization of cost for differentiated recovery. The cost model is built with tuning parameters that can be used to customize the strategy for different failure rates in different computing environments. We also show that this approach can make recovery cost proportional to the failure rate.","tags":null,"title":"Granularity and the cost of error recovery in resilient AMR scientific applications","type":"publication"},{"authors":["Qingrui Liu","Changhee Jung","Dongyoon Lee","Devesh Tiwari"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"40d9c7d591281c83f4774280ffd0f010","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confmicro-liu-jlt-16/","publishdate":"2020-07-24T21:05:21.282046Z","relpermalink":"/publication/dblp-confmicro-liu-jlt-16/","section":"publication","summary":"This paper presents Turnstile, a hardware/software cooperative technique for low-cost soft error resilience. Leveraging the recent advance of acoustic sensor based soft error detection, Turnstile achieves guaranteed recovery by taking into account the bounded detection latency. The compiler forms verifiable regions and selectively inserts store instructions to checkpoint their register inputs so that Turnstile can verify the register/memory states with regard to a region boundary in a unified way without expensive register file protection. At runtime, for each region, Turnstile regards any stores (to both memory and register checkpoints) as unverified, and thus holds them in a store queue until the region ends and spends the time of the error detection latency. If no error is detected during the time, the verified stores are merged into memory systems, and registers are checkpointed. When all the stores including checkpointing stores prior to a region boundary are verified, the architectural and memory states with regard to the boundary are verified, thus it can serve as a recovery point. In this way, Turnstile contains the errors within the core without extra memory buffering. When an error is detected, Turnstile invalidates unverified entries in the store queue and restores the checkpointed register values to get the architectural and memory states back to what they were at the most recently verified region boundary. Then, Turnstile simply redirects program control to the verified region boundary and continues execution. The experimental results demonstrate that Turnstile can offer guaranteed soft error recovery with low performance overhead (","tags":null,"title":"Low-cost soft error resilience with unified data verification and fine-grained recovery for acoustic sensor based detection","type":"publication"},{"authors":["Kun Tang","Devesh Tiwari","Saurabh Gupta","Ping Huang","Qiqi Lu","Christian Engelmann","Xubin He"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"2f05f2520c11c41207fef650a9d489de","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confdsn-tang-tghleh-16/","publishdate":"2020-07-24T21:05:21.279044Z","relpermalink":"/publication/dblp-confdsn-tang-tghleh-16/","section":"publication","summary":"Checkpoint and restart mechanisms have been widely used in large scientific simulation applications to make forward progress in case of failures. However, none of the prior works have considered the interaction of power-constraint with temperature, reliability, performance, and checkpointing interval. It is not clear how power-capping may affect optimal checkpointing interval. What are the involved reliability, performance, and energy trade-offs? In this paper, we develop a deep understanding about the interaction between power-capping and scientific applications using checkpoint/restart as resilience mechanism, and propose a new model for the optimal checkpointing interval (OCI) under power-capping. Our study reveals several interesting, and previously unknown, insights about how power-capping affects the reliability, energy consumption, performance.","tags":null,"title":"Power-Capping Aware Checkpointing: On the Interplay Among Power-Capping, Temperature, Reliability, Performance, and Energy","type":"publication"},{"authors":["Leonardo Arturo Bautista-Gomez","Ana Gainaru","Swann Perarnau","Devesh Tiwari","Saurabh Gupta","Christian Engelmann","Franck Cappello","Marc Snir"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"3a11f4160c2ce0d1bbe3fe0bde0fb24f","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confipps-bautista-gomez-g-16/","publishdate":"2020-07-24T21:05:21.281045Z","relpermalink":"/publication/dblp-confipps-bautista-gomez-g-16/","section":"publication","summary":"Resilience is an important challenge for extreme-scale supercomputers. Today, failures in supercomputers are assumed to be uniformly distributed in time. However, recent studies show that failures in high-performance computing systems are partially correlated in time, generating periods of higher failure density. Our study of the failure logs of multiple supercomputers show that periods of higher failure density occur with up to three times more than the average. We design a monitoring system that listens to hardware events and forwards important events to the runtime to detect those regime changes. We implement a runtime capable of receiving notifications and adapt dynamically. In addition, we build an analytical model to predict the gains that such dynamic approach could achieve. We demonstrate that in some systems, our approach can reduce the wasted time by over 30%.","tags":null,"title":"Reducing Waste in Extreme Scale Systems through Introspective Analysis","type":"publication"},{"authors":["Lipeng Wan","Feiyi Wang","Sarp Oral","Devesh Tiwari","Sudharshan S. Vazhkudai","Qing Cao"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"50af92f8187276add266cff64040c6fe","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confsc-wan-wotvc-15/","publishdate":"2020-07-24T21:05:21.290054Z","relpermalink":"/publication/dblp-confsc-wan-wotvc-15/","section":"publication","summary":"The increasing data demands from high-performance computing applications significantly accelerate the capacity, capability and reliability requirements of storage systems. As systems scale, component failures and repair times increase, significantly impacting data availability. A wide array of decision points must be balanced in designing such systems.\n We propose a systematic approach that balances and optimizes both initial and continuous spare provisioning based on a detailed investigation of the anatomy and field failure data analysis of extreme-scale storage systems. We consider the component failure characteristics and its cost and impact at the system level simultaneously. We build a tool to evaluate different provisioning schemes, and the results demonstrate that our optimized provisioning can reduce the duration of data unavailability by as much as 52% under a fixed budget. We also observe that non-disk components have much higher failure rates than disks, and warrant careful considerations in the overall provisioning process.","tags":null,"title":"A practical approach to reconciling availability, performance, and capacity in provisioning extreme-scale storage systems","type":"publication"},{"authors":["Hyogi Sim","Youngjae Kim","Sudharshan S. Vazhkudai","Devesh Tiwari","Ali Anwar","Ali Raza Butt","Lavanya Ramakrishnan"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"b1d96700670f1de39034b91e5d87e504","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confsc-sim-kvtabr-15/","publishdate":"2020-07-24T21:05:21.288052Z","relpermalink":"/publication/dblp-confsc-sim-kvtabr-15/","section":"publication","summary":"The need for novel data analysis is urgent in the face of a data deluge from modern applications. Traditional approaches to data analysis incur significant data movement costs, moving data back and forth between the storage system and the processor. Emerging Active Flash devices enable processing on the flash, where the data already resides. An array of such Active Flash devices allows us to revisit how analysis workflows interact with storage systems. By seamlessly blending together the flash storage and data analysis, we create an analysis workflow-aware storage system, AnalyzeThis. Our guiding principle is that analysis-awareness be deeply ingrained in each and every layer of the storage, elevating data analyses as first-class citizens, and transforming AnalyzeThis into a potent analytics-aware appliance. We implement the AnalyzeThis storage system atop an emulation platform of the Active Flash array. Our results indicate that AnalyzeThis is viable, expediting workflow execution and minimizing data movement.","tags":null,"title":"AnalyzeThis: an analysis workflow-aware storage system","type":"publication"},{"authors":["Qingrui Liu","Changhee Jung","Dongyoon Lee","Devesh Tiwari"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"0d1caab2e564127bf3011332f1c5f254","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-conflctrts-liu-jlt-15/","publishdate":"2020-07-24T21:05:21.287051Z","relpermalink":"/publication/dblp-conflctrts-liu-jlt-15/","section":"publication","summary":"This paper presents Clover, a compiler directed soft error detection and recovery scheme for lightweight soft error resilience. The compiler carefully generates soft error tolerant code based on idempotent processing without explicit checkpoint. During program execution, Clover relies on a small number of acoustic wave detectors deployed in the processor to identify soft errors by sensing the wave made by a particle strike. To cope with DUE (detected unrecoverable errors) caused by the sensing latency of error detection, Clover leverages a novel selective instruction duplication technique called tail-DMR (dual modular redundancy). Once a soft error is detected by either the sensor or the tail-DMR, Clover takes care of the error as in the case of exception handling. To recover from the error, Clover simply redirects program control to the beginning of the code region where the error is detected. The experiment results demonstrate that the average runtime overhead is only 26%, which is a 75% reduction compared to that of the state-of-the-art soft error resilience technique.","tags":null,"title":"Clover: Compiler Directed Lightweight Soft Error Resilience","type":"publication"},{"authors":["Ruijun Wang","Devesh Tiwari","Jun Wang"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"8f3f1420156224d6f210b9694128096d","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confdsdis-wang-tw-15/","publishdate":"2020-07-24T21:05:21.284048Z","relpermalink":"/publication/dblp-confdsdis-wang-tw-15/","section":"publication","summary":"Supercomputer's fast processing speed provides a great convenience to the scientists who dealing with extremely large data sets. The next generation of \"exascale\" supercomputers could provide accurate simulation results in the area of automobile industry, aerospace and even nuclear fusion reactors for the very first time. However, the energy cost of super-computing is \"super\" expensive with a total electricity bill of 9 million dollars per year. Thus, Conserving energy or increase the energy efficiency are becoming more critical. Many researchers are looking into this problem and try to conserve energy by incorporating DVFS technique into their specific methods. However, this approach is limited especially when the workload is high. In this paper, we developed a power-aware job scheduler by applying rule based control method as well as real power and speedup profiles to improve power efficiency while maintain the power constraints. The intensive simulation results shown that our proposed method is able to achieve the maximum utilization of co   mputing resources, in the meantime, keep the energy cost under the threshold. Moreover, by introducing a Power Performance Factor (PPF) based on the real power and speedup profiles, we are able to increase the power efficiency up to 75%.","tags":null,"title":"Low Power Job Scheduler for Supercomputers: A Rule-Based Power-Aware Scheduler","type":"publication"},{"authors":["Jaimie Kelley","Christopher Stewart","Nathaniel Morris","Devesh Tiwari","Yuxiong He","Sameh Elnikety"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"5868dd62c37486b55bab747ba30c8a82","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-conficac-kelley-smthe-15/","publishdate":"2020-07-24T21:05:21.287051Z","relpermalink":"/publication/dblp-conficac-kelley-smthe-15/","section":"publication","summary":"Online data-intensive services parallelize query execution across distributed software components. Interactive response time is a priority, so online query executions return answers without waiting for slow running components to finish. However, data from these slow components could lead to better answers. We propose Ubora, an approach to measure the effect of slow running components on the quality of answers. Ubora randomly samples online queries and executes them twice. The first execution elides data from slow components and provides fast online answers, the second execution waits for all components to complete. Ubora uses memoization to speed up mature executions by replaying network messages exchanged between components. Our systems-level implementation works for a wide range of platforms, including Hadoop/Yarn, Apache Lucene, the Easy Rec Recommendation Engine, and the Open Ephyra question answering system. Ubora computes answer quality much faster than competing approaches that do not use memoization. With Ubora, we show that answer quality can and should be used to guide online admission control. Our adaptive controller processed 37% more queries than a competing controller guided by the rate of timeouts.","tags":null,"title":"Measuring and Managing Answer Quality for Online Data-Intensive Services","type":"publication"},{"authors":["Devesh Tiwari","Saurabh Gupta","George Gallarno","Jim Rogers","Don Maxwell"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"47eb3c9e24ed66f2d4ba25aa8312a103","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confsc-tiwari-ggrm-15/","publishdate":"2020-07-24T21:05:21.290054Z","relpermalink":"/publication/dblp-confsc-tiwari-ggrm-15/","section":"publication","summary":"The high computational capability of graphics processing units (GPUs) is enabling and driving the scientific discovery process at large-scale. The world's second fastest supercomputer for open science, Titan, has more than 18,000 GPUs that computational scientists use to perform scientific simulations and data analysis. Understanding of GPU reliability characteristics, however, is still in its nascent stage since GPUs have only recently been deployed at large-scale. This paper presents a detailed study of GPU errors and their impact on system operations and applications, describing experiences with the 18,688 GPUs on the Titan supercomputer as well as lessons learned in the process of efficient operation of GPUs at scale. These experiences are helpful to HPC sites which already have large-scale GPU clusters or plan to deploy GPUs in the future.","tags":null,"title":"Reliability lessons learned from GPU experience with the Titan supercomputer at Oak Ridge leadership computing facility","type":"publication"},{"authors":["Saurabh Gupta","Devesh Tiwari","Christopher Jantzi","James H. Rogers","Don Maxwell"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"0cca59290cd70a9436bbf8fdd9c21e7a","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confdsn-gupta-tjrm-15/","publishdate":"2020-07-24T21:05:21.285049Z","relpermalink":"/publication/dblp-confdsn-gupta-tjrm-15/","section":"publication","summary":"As we approach exascale, the scientific simulations are expected to experience more interruptions due to increased system failures. Designing better HPC resilience techniques requires understanding the key characteristics of system failures on these systems. While temporal properties of system failures on HPC systems have been well-investigated, there is limited understanding about the spatial characteristics of system failures and its impact on the resilience mechanisms. Therefore, we examine the spatial characteristics and behavior of system failures. We investigate the interaction between spatial and temporal characteristics of failures and its implications for system operations and resilience mechanisms on large-scale HPC systems. We show that system failures have 'spatial locality' at different granularity in the system, study impact of different failure-types, and investigate the correlation among different failure-types. Finally, we propose a novel scheme that exploits the spatial locality in failures to improve application and system performance. Our evaluation shows that the proposed scheme significantly improves the system performance in a dynamic and production-level HPC system.","tags":null,"title":"Understanding and Exploiting Spatial Properties of System Failures on Extreme-Scale HPC Systems","type":"publication"},{"authors":["Devesh Tiwari","Saurabh Gupta","James H. Rogers","Don Maxwell","Paolo Rech","Sudharshan S. Vazhkudai","Daniel A. G. de Oliveira","Dave Londo","Nathan DeBardeleben","Philippe Olivier Alexandre Navaux","Luigi Carro","Arthur S. Bland"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"e992cac5c1bc87a1c6374fe71b62b2f0","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confhpca-tiwari-grmrvoldn-15/","publishdate":"2020-07-24T21:05:21.28605Z","relpermalink":"/publication/dblp-confhpca-tiwari-grmrvoldn-15/","section":"publication","summary":"Increase in graphics hardware performance and improvements in programmability has enabled GPUs to evolve from a graphics-specific accelerator to a general-purpose computing device. Titan, the world's second fastest supercomputer for open science in 2014, consists of more dum 18,000 GPUs that scientists from various domains such as astrophysics, fusion, climate, and combustion use routinely to run large-scale simulations. Unfortunately, while the performance efficiency of GPUs is well understood, their resilience characteristics in a large-scale computing system have not been fully evaluated. We present a detailed study to provide a thorough understanding of GPU errors on a large-scale GPU-enabled system. Our data was collected from the Titan supercomputer at the Oak Ridge Leadership Computing Facility and a GPU cluster at the Los Alamos National Laboratory. We also present results from our extensive neutron-beam tests, conducted at Los Alamos Neutron Science Center (LANSCE) and at ISIS (Rutherford Appleron Laboratories, UK), to measure the resilience of different generations of GPUs. We present several findings from our field data and neutron-beam experiments, and discuss the implications of our results for future GPU architects, current and future HPC computing facilities, and researchers focusing on GPU resilience.","tags":null,"title":"Understanding GPU errors on large-scale HPC systems and the implications for system design and operation","type":"publication"},{"authors":["Sarp Oral","James Simmons","Jason Hill","Dustin Leverman","Feiyi Wang","Matthew A. Ezell","Ross G. Miller","Douglas Fuller","Raghul Gunasekaran","Youngjae Kim","Saurabh Gupta","Devesh Tiwari","Sudharshan S. Vazhkudai","James H. Rogers","David Dillow","Galen M. Shipman","Arthur S. Bland"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"cf5938e94f4f40c1d04d0944f4bd2619","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confsc-oral-shlwemfgkgtvrdsb-14/","publishdate":"2020-07-24T21:05:21.294058Z","relpermalink":"/publication/dblp-confsc-oral-shlwemfgkgtvrdsb-14/","section":"publication","summary":"The Oak Ridge Leadership Computing Facility (OLCF) has deployed multiple large-scale parallel file systems (PFS) to support its operations. During this process, OLCF acquired significant expertise in large-scale storage system design, file system software development, technology evaluation, benchmarking, procurement, deployment, and operational practices. Based on the lessons learned from each new PFS deployment, OLCF improved its operating procedures, and strategies. This paper provides an account of our experience and lessons learned in acquiring, deploying, and operating large-scale parallel file systems. We believe that these lessons will be useful to the wider HPC community.","tags":null,"title":"Best Practices and Lessons Learned from Deploying and Operating Large-Scale Data-Centric Parallel File Systems","type":"publication"},{"authors":["Feiyi Wang","Sarp Oral","Saurabh Gupta","Devesh Tiwari","Sudharshan S. Vazhkudai"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"3ff7c89bfead2ddfc6d68bcc80546a5f","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-conficpads-wang-ogtv-14/","publishdate":"2020-07-24T21:05:21.293057Z","relpermalink":"/publication/dblp-conficpads-wang-ogtv-14/","section":"publication","summary":"With the advent of big data, the I/O subsystems of large-scale compute clusters are becoming a center of focus. More applications are putting greater demands on end-to-end I/O performance. These subsystems are often complex in design. They comprise of multiple hardware and software layers to cope with the increasing capacity, capability, and scalability requirements of data intensive applications. However, the sharing nature of storage resources and the intrinsic interactions across these layers make it a great challenge to realize end-to-end performance gains. This paper proposes a topology-aware strategy to balance the load across resources, to improve the per-application I/O performance. We demonstrate the effectiveness of our algorithm on an extreme-scale compute cluster, Titan, at the Oak Ridge Leadership Computing Facility (OLCF). Our experiments with both synthetic benchmarks and a real-world application show that, even under congestion, our proposed algorithm can improve large-scale application I/O performance significantly, resulting in both a reduction in application run time as well as a higher resolution of simulation run.","tags":null,"title":"Improving large-scale storage system performance via topology-aware and balanced data placement","type":"publication"},{"authors":["Devesh Tiwari","Saurabh Gupta","Sudharshan S. Vazhkudai"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"c4d79f51931e53c26b7747868344c305","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confdsn-tiwari-gv-14/","publishdate":"2020-07-24T21:05:21.292055Z","relpermalink":"/publication/dblp-confdsn-tiwari-gv-14/","section":"publication","summary":"Continuing increase in the computational power of supercomputers has enabled large-scale scientific applications in the areas of astrophysics, fusion, climate and combustion to run larger and longer-running simulations, facilitating deeper scientific insights. However, these long-running simulations are often interrupted by multiple system failures. Therefore, these applications rely on \"check pointing\" as a resilience mechanism to store application state to permanent storage and recover from failures. Unfortunately, check pointing incurs excessive I/O overhead on supercomputers due to large size of checkpoints, resulting in a sub-optimal performance and resource utilization. In this paper, we devise novel mechanisms to show how check pointing overhead can be mitigated significantly by exploiting the temporal characteristics of system failures. We provide new insights and detailed quantitative understanding of the check pointing overheads and trade-offs on large-scale machines. Our prototype implementation shows the viability of our approach on extreme-scale machines.","tags":null,"title":"Lazy Checkpointing: Exploiting Temporal Locality in Failures to Mitigate Checkpointing Overheads on Extreme-Scale Systems","type":"publication"},{"authors":["Devesh Tiwari","Yan Solihin"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"cd3fd4c319cabe920725c9783ed807c7","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confipps-tiwari-s-14/","publishdate":"2020-07-24T21:05:21.293057Z","relpermalink":"/publication/dblp-confipps-tiwari-s-14/","section":"publication","summary":"MapReduce programming model is being increasingly adopted for data intensive high performance computing. Recently, it has been observed that in data-intensive environment, programs are often run multiple times with either identical or slightly-changed input, which creates a significant opportunity for computation reuse. Recognizing the opportunity, researchers have proposed techniques to reuse computation in disk-based MapReduce systems such as Hadoop, but not for in-memory MapReduce (IMMR) systems such as Phoenix. In this paper, we propose a novel technique for computation reuse in IMMR systems, which we refer to as MapReuse. MapReuse detects input similarity by comparing their signatures. It skips re-computing output from a repeated portion of the input, computes output from a new portion of input, and removes output that corresponds to a deleted portion of the input. MapReuse is built on top of an existing IMMR system, leaving it largely unmodified. MapReuse significantly speeds up IMMR, even when the new input differs by 25% compared to the original input. ","tags":null,"title":"MapReuse: Reusing Computation in an In-Memory MapReduce System","type":"publication"},{"authors":["Devesh Tiwari","Simona Boboila","Sudharshan S. Vazhkudai","Youngjae Kim","Xiaosong Ma","Peter Desnoyers","Yan Solihin"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"a5bc6e1c0c077a9075e6cb41aac7b023","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-conffast-tiwari-bvkmds-13/","publishdate":"2020-07-24T21:05:21.295058Z","relpermalink":"/publication/dblp-conffast-tiwari-bvkmds-13/","section":"publication","summary":"Modern scientiﬁc discovery is increasingly driven by large-scale supercomputing simulations, followed by data analysis tasks. These data analyses are either performed ofﬂine, on smaller-scale clusters, or on the supercomputer itself. Unfortunately, these techniques suffer from performance and energy inefﬁciencies due to increased data movement between the compute and storage subsystems. Therefore, we propose Active Flash, an insitu scientiﬁc data analysis approach, wherein data analysis is conducted on the solid-state device (SSD), wherethe data already resides. Our performance and energy models show that Active Flash has the potential to address many of the aforementioned concerns without degrading HPC simulation performance. In addition, we demonstrate an Active Flash prototype built on a commercial SSD controller, which further reafﬁrms the viability of our proposal.","tags":null,"title":"Active flash: towards energy-efficient, in-situ data analytics on extreme-scale machines","type":"publication"},{"authors":["Devesh Tiwari","Yan Solihin"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"53380a0af20a0485d548449a46ff01a2","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confispass-tiwari-s-12/","publishdate":"2020-07-24T21:05:21.29706Z","relpermalink":"/publication/dblp-confispass-tiwari-s-12/","section":"publication","summary":"Today, more than 99% of web-browsers are enabled with Javascript capabilities, and Javascript's popularity is only going to increase in the future. However, due to bytecode interpretation, Javascript codes suffer from severe performance penalty (up to 50x slower) compared to the corresponding native C/C++ code. We recognize that the first step to bridge this performance gap is to understand the the architectural execution characteristics of Javascript benchmarks. Therefore, this paper presents an in-depth architectural characterization of widely used V8 and Sunspider Javascript benchmarks using Google's V8 javascript engine. Using statistical data analysis techniques, our characterization study discovers and explains correlation among different execution characteristics in microarchitecture dependent as well as microarchitecture independent fashion. Furthermore, our study measures (dis)similarity among 33 different Javascript benchmarks and discusses its implications. Given the widespread use of Javascripts, we believe our findings are useful for both performance analysis and benchmarking communities.","tags":null,"title":"Architectural characterization and similarity analysis of sunspider and Google's V8 Javascript benchmarks","type":"publication"},{"authors":["Devesh Tiwari","Yan Solihin"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"d93999b36fee4da383e23da987b02855","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confipps-tiwari-s-12/","publishdate":"2020-07-24T21:05:21.296059Z","relpermalink":"/publication/dblp-confipps-tiwari-s-12/","section":"publication","summary":"MapReduce parallel programming model has seen wide adoption in data center applications. Recently, lightweight, fast, in-memory MapReduce runtime systems have been proposed for shared memory systems. However, what factors affect performance and what performance bottlenecks exist for a given program, are not well understood. This paper builds an analytical model to capture key performance factors of shared memory MapReduce and investigates important performance trends and behavior. Our study discovers several important findings and implications for system designers, performance tuners, and programmers. Our model quantifies relative contribution of different key performance factors for both map and reduce phases, and shows that performance of MapReduce programs are highly input-content dependent. Our model reveals that performance is heavily affected by the order in which distinct keys are encountered during the Map phase, and the frequency of these distinct keys. Our model points out cases in which reduce phase time dominates the total execution time. We also show that data-structure and algorithm design choices affect map and reduce phases differently and sometimes affecting map phase positively while affecting reduce phase negatively. Finally, we propose an application classification framework that can be used to reason about performance bottlenecks for a given application.","tags":null,"title":"Modeling and Analyzing Key Performance Factors of Shared Memory MapReduce","type":"publication"},{"authors":["Devesh Tiwari","Sudharshan S. Vazhkudai","Youngjae Kim","Xiaosong Ma","Simona Boboila","Peter Desnoyers"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"4e97afe1122b36ad4487ebf4e5d7cd5c","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confosdi-tiwari-vkmbd-12/","publishdate":"2020-07-24T21:05:21.29706Z","relpermalink":"/publication/dblp-confosdi-tiwari-vkmbd-12/","section":"publication","summary":" Modern scientific discovery often involves running complex application simulations on supercomputers, followed by a sequence of data analysis tasks on smaller clusters. This offline approach suffers from significant data movement costs such as redundant I/O, storage bandwidth bottleneck, and wasted CPU cycles, all of which contribute to increased energy consumption and delayed end-to- end performance. Technology projections for an exascale machine indicate that energy-efficiency will become the primary design metric. It is estimated that the energy cost of data movement will soon rival the cost of computation. Consequently, we can no longer ignore the data movement costs in data analysis. To address these challenges, we advocate executing data analysis tasks on emerging storage devices, such as SSDs. Typically, in extreme-scale systems, SSDs serve only as a temporary storage system for the simulation output data. In our approach, Active Flash, we propose to conduct in-situ data analysis on the SSD controller without degrading the performance of the simulation job. By migrating analysis tasks closer to where the data resides, it helps reduce the data movement cost. We present detailed energy and performance models for both active flash and offline strategies, and study them using extreme-scale application simulations, commonly used data analytics kernels, and supercomputer system configurations. Our evaluation suggests that active flash is a promising approach to alleviate the storage bandwidth bottleneck, reduce the data movement cost, and improve the overall energy efficiency.","tags":null,"title":"Reducing Data Movement Costs Using Energy-Efficient, Active Computation on SSD","type":"publication"},{"authors":["Sanghoon Lee","Devesh Tiwari","Yan Solihin","James Tuck"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"cd5c657ab38159c7c6ae5e828001b97c","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confhpca-lee-tst-11/","publishdate":"2020-07-24T21:05:21.298061Z","relpermalink":"/publication/dblp-confhpca-lee-tst-11/","section":"publication","summary":"Queues are commonly used in multithreaded programs for synchronization and communication. However, because software queues tend to be too expensive to support finegrained parallelism, hardware queues have been proposed to reduce overhead of communication between cores. Hardware queues require modifications to the processor core and need a custom interconnect. They also pose difficulties for the operating system because their state must be preserved across context switches. To solve these problems, we propose a hardware-accelerated queue, or HAQu. HAQu adds hardware to a CMP that accelerates operations on software queues. Our design implements fast queueing through an application's address space with operations that are compatible with a fully software queue. Our design provides accelerated and OS-transparent performance in three general ways: (1) it provides a single instruction for enqueueing and dequeueing which significantly reduces the overhead when used in fine-grained threading; (2) operations on the queue are designed to leverage low-level details of the coherence protocol; and (3) hardware ensures that the full state of the queue is stored in the application's address space, thereby ensuring virtualization. We have evaluated our design in the context of application domains: offloading fine-grained checks for improved software reliability, and automatic, fine-grained parallelization using decoupled software pipelining.","tags":null,"title":"HAQu: Hardware-accelerated queueing for fine-grained threading on a chip multiprocessor","type":"publication"},{"authors":["Devesh Tiwari","Sanghoon Lee","James Tuck","Yan Solihin"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"82bbb031d4b977fba8ef88ab2365be2b","permalink":"https://goodwillcomputinglab.github.io/publication/dblp-confipps-tiwari-lts-10/","publishdate":"2020-07-24T21:05:21.299062Z","relpermalink":"/publication/dblp-confipps-tiwari-lts-10/","section":"publication","summary":"Dynamic memory management is one of the most expensive but ubiquitous operations in many C/C++ applications. Additional features such as security checks, while desirable, further worsen memory management overheads. With advent of multicore architecture, it is important to investigate how dynamic memory management overheads for sequential applications can be reduced. In this paper, we propose a new approach for accelerating dynamic memory management on multicore architecture, by offloading dynamic management functions to a separate thread that we refer to as memory management thread (MMT). We show that an efficient MMT design can give significant performance improvement by extracting parallelism while being agnostic to the underlying memory management library algorithms and data structures. We also show how parallelism provided by MMT can be beneficial for high overhead memory management tasks, for example, security checks related to memory management. We evaluate MMT on heap allocation-intensive benchmarks running on an Intel core 2 quad platform for two widely-used memory allocators: Doug Lea's and PHKmalloc allocators. On average, MMT achieves a speedup ratio of 1.19× for both allocators, while both the application and memory management libraries are unmodified and are oblivious to the parallelization scheme. For PHKmalloc with security checks turned on, MMT reduces the security check overheads from 21% to only 1% on average.","tags":null,"title":"MMT: Exploiting fine-grained parallelism in dynamic memory management","type":"publication"}]