<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Abhay Potharaju">

  
  
  
    
  
  <meta name="description" content="Masters Student">

  
  <link rel="alternate" hreflang="en-us" href="https://goodwillcomputinglab.github.io/publication/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      
        
      

      
    
      

      
      

      
    

  

  
  
  
    
      
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
    
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  
  

  
  <link rel="alternate" href="/publication/index.xml" type="application/rss+xml" title="Goodwill Computing Lab">
  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu45ac71144705710c3d7ad8da46e18545_4491_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu45ac71144705710c3d7ad8da46e18545_4491_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://goodwillcomputinglab.github.io/publication/">

  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Goodwill Computing Lab">
  <meta property="og:url" content="https://goodwillcomputinglab.github.io/publication/">
  <meta property="og:title" content="All Publications | Goodwill Computing Lab">
  <meta property="og:description" content="Masters Student"><meta property="og:image" content="https://goodwillcomputinglab.github.io/images/icon_hu45ac71144705710c3d7ad8da46e18545_4491_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="https://goodwillcomputinglab.github.io/images/icon_hu45ac71144705710c3d7ad8da46e18545_4491_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us">
  
    <meta property="og:updated_time" content="2020-09-21T00:00:00&#43;00:00">
  

  




  


  





  <title>All Publications | Goodwill Computing Lab</title>

</head>
<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  









<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-fluid">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Goodwill Computing Lab</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Goodwill Computing Lab</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/people"><span>Team</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/research/"><span>Research</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/recent-publication/"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/outreach"><span>Educational Outreach</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/artifacts"><span>Open-Source Artifacts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/news/"><span>News</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/contact"><span>Join Us</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link js-theme-selector" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-palette" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>


  


<div class="header">
  <h1>All Publications</h1>
</div>



  <div class="row">
    <div class="col-lg-12">

      

      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

      
      

      <div id="container-publications">
        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-2 year-2020">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [SC 2020 (a)]
            
          
        </span>
        <a href="https://doi.org/10.1109/TPDS.2018.2865471" class="article-title" 
          style="color:inherit; font-weight: normal;">Experimental Evaluation of NISQ Quantum Computers: Error Measurement, Characterization, and Implications (Best Paper Finalist, Best Student Paper Finalist)</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-journalstpds-temp\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/TPDS.2018.2865471" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-journalstpds-temp\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-journalstpds-temp/-abstract" style="display: none;">
            <p class="abstract abstract-text">The need for novel data analysis is urgent in the face of a data deluge from modern applications. Traditional approaches to data analysis incur significant data movement costs, moving data back and forth between the storage system and the processor. Emerging Active Flash devices enable processing on the flash, where the data already resides. An array of such Active Flash devices allows us to revisit how analysis workflows interact with storage systems. By seamlessly blending together the flash storage and data analysis, we create an analysis workflow-aware storage system, AnalyzeThis. Our guiding principle is that analysis-awareness be deeply ingrained in each and every layer of the storage, elevating data analyses as first-class citizens, and transforming AnalyzeThis into a potent analytics-aware appliance. To evaluate the AnalyzeThis system, we have adopted both emulation and simulation approaches. In particular, we have evaluated AnalyzeThis by implementing the AnalyzeThis storage system on top of the Active Flash Array's emulation platform. We have also implemented an event-driven AnalyzeThis simulator, called AnalyzeThisSim, which allows us to address the limitations of the emulation platform, e.g., performance impact of using multi-core SSDs. The results from our emulation and simulation platforms indicate that AnalyzeThis is a viable approach for expediting workflow execution and minimizing data movement.</p>
          </div>
          
          
          <div id="/publication/dblp-journalstpds-temp/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-journalstpds-temp/-cite" class="tex hljs" data-filename="/publication/dblp-journalstpds-temp/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-2 year-2020">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [SC 2020 (b)]
            
          
        </span>
        <a href="https://doi.org/10.1109/TPDS.2018.2865471" class="article-title" 
          style="color:inherit; font-weight: normal;">VERITAS: Accurately Estimating the Correct Output on Noisy Intermediate-Scale Quantum Computers</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/temp2\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/TPDS.2018.2865471" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/temp2\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/temp2/-abstract" style="display: none;">
            <p class="abstract abstract-text">The need for novel data analysis is urgent in the face of a data deluge from modern applications. Traditional approaches to data analysis incur significant data movement costs, moving data back and forth between the storage system and the processor. Emerging Active Flash devices enable processing on the flash, where the data already resides. An array of such Active Flash devices allows us to revisit how analysis workflows interact with storage systems. By seamlessly blending together the flash storage and data analysis, we create an analysis workflow-aware storage system, AnalyzeThis. Our guiding principle is that analysis-awareness be deeply ingrained in each and every layer of the storage, elevating data analyses as first-class citizens, and transforming AnalyzeThis into a potent analytics-aware appliance. To evaluate the AnalyzeThis system, we have adopted both emulation and simulation approaches. In particular, we have evaluated AnalyzeThis by implementing the AnalyzeThis storage system on top of the Active Flash Array's emulation platform. We have also implemented an event-driven AnalyzeThis simulator, called AnalyzeThisSim, which allows us to address the limitations of the emulation platform, e.g., performance impact of using multi-core SSDs. The results from our emulation and simulation platforms indicate that AnalyzeThis is a viable approach for expediting workflow execution and minimizing data movement.</p>
          </div>
          
          
          <div id="/publication/temp2/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/temp2/-cite" class="tex hljs" data-filename="/publication/temp2/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-2 year-2020">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [SC 2020 (c)]
            
          
        </span>
        <a href="https://doi.org/10.1109/TPDS.2018.2865471" class="article-title" 
          style="color:inherit; font-weight: normal;">Job Characteristics on Large-Scale Systems: Long-Term Analysis, Quantification and Implications</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/temp3\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/TPDS.2018.2865471" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/temp3\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/temp3/-abstract" style="display: none;">
            <p class="abstract abstract-text">The need for novel data analysis is urgent in the face of a data deluge from modern applications. Traditional approaches to data analysis incur significant data movement costs, moving data back and forth between the storage system and the processor. Emerging Active Flash devices enable processing on the flash, where the data already resides. An array of such Active Flash devices allows us to revisit how analysis workflows interact with storage systems. By seamlessly blending together the flash storage and data analysis, we create an analysis workflow-aware storage system, AnalyzeThis. Our guiding principle is that analysis-awareness be deeply ingrained in each and every layer of the storage, elevating data analyses as first-class citizens, and transforming AnalyzeThis into a potent analytics-aware appliance. To evaluate the AnalyzeThis system, we have adopted both emulation and simulation approaches. In particular, we have evaluated AnalyzeThis by implementing the AnalyzeThis storage system on top of the Active Flash Array's emulation platform. We have also implemented an event-driven AnalyzeThis simulator, called AnalyzeThisSim, which allows us to address the limitations of the emulation platform, e.g., performance impact of using multi-core SSDs. The results from our emulation and simulation platforms indicate that AnalyzeThis is a viable approach for expediting workflow execution and minimizing data movement.</p>
          </div>
          
          
          <div id="/publication/temp3/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/temp3/-cite" class="tex hljs" data-filename="/publication/temp3/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-2 year-2020">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [ICCAD 2020]
            
          
        </span>
        <a href="https://doi.org/10.1109/TPDS.2018.2865471" class="article-title" 
          style="color:inherit; font-weight: normal;">DISQ: A Novel Quantum Output State Classification Method on IBM Quantum Computers using OpenPulse (Best Paper Finalist)</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/temp4\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/TPDS.2018.2865471" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/temp4\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/temp4/-abstract" style="display: none;">
            <p class="abstract abstract-text">The need for novel data analysis is urgent in the face of a data deluge from modern applications. Traditional approaches to data analysis incur significant data movement costs, moving data back and forth between the storage system and the processor. Emerging Active Flash devices enable processing on the flash, where the data already resides. An array of such Active Flash devices allows us to revisit how analysis workflows interact with storage systems. By seamlessly blending together the flash storage and data analysis, we create an analysis workflow-aware storage system, AnalyzeThis. Our guiding principle is that analysis-awareness be deeply ingrained in each and every layer of the storage, elevating data analyses as first-class citizens, and transforming AnalyzeThis into a potent analytics-aware appliance. To evaluate the AnalyzeThis system, we have adopted both emulation and simulation approaches. In particular, we have evaluated AnalyzeThis by implementing the AnalyzeThis storage system on top of the Active Flash Array's emulation platform. We have also implemented an event-driven AnalyzeThis simulator, called AnalyzeThisSim, which allows us to address the limitations of the emulation platform, e.g., performance impact of using multi-core SSDs. The results from our emulation and simulation platforms indicate that AnalyzeThis is a viable approach for expediting workflow execution and minimizing data movement.</p>
          </div>
          
          
          <div id="/publication/temp4/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/temp4/-cite" class="tex hljs" data-filename="/publication/temp4/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-2 year-2020">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            [USENIX ATC 2020]
          
        </span>
        <a href="https://doi.org/10.1109/TPDS.2018.2865471" class="article-title" 
          style="color:inherit; font-weight: normal;">UREQA: Leveraging Operation-Aware Error Rates for Effective Quantum Circuit Mapping on NISQ-Era Quantum Computers</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/temp5\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/TPDS.2018.2865471" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/temp5\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/temp5/-abstract" style="display: none;">
            <p class="abstract abstract-text">The need for novel data analysis is urgent in the face of a data deluge from modern applications. Traditional approaches to data analysis incur significant data movement costs, moving data back and forth between the storage system and the processor. Emerging Active Flash devices enable processing on the flash, where the data already resides. An array of such Active Flash devices allows us to revisit how analysis workflows interact with storage systems. By seamlessly blending together the flash storage and data analysis, we create an analysis workflow-aware storage system, AnalyzeThis. Our guiding principle is that analysis-awareness be deeply ingrained in each and every layer of the storage, elevating data analyses as first-class citizens, and transforming AnalyzeThis into a potent analytics-aware appliance. To evaluate the AnalyzeThis system, we have adopted both emulation and simulation approaches. In particular, we have evaluated AnalyzeThis by implementing the AnalyzeThis storage system on top of the Active Flash Array's emulation platform. We have also implemented an event-driven AnalyzeThis simulator, called AnalyzeThisSim, which allows us to address the limitations of the emulation platform, e.g., performance impact of using multi-core SSDs. The results from our emulation and simulation platforms indicate that AnalyzeThis is a viable approach for expediting workflow execution and minimizing data movement.</p>
          </div>
          
          
          <div id="/publication/temp5/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/temp5/-cite" class="tex hljs" data-filename="/publication/temp5/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2020">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [FAST 2020 (a)]
            
          
        </span>
        <a href="https://www.usenix.org/conference/fast20/presentation/patel-hpc-systems" class="article-title" 
          style="color:inherit; font-weight: normal;">Uncovering Access, Reuse, and Sharing Characteristics of I/O-Intensive Files on Large-Scale Production HPC Systems</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-conffast-patel-blwcrt-20\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://www.usenix.org/conference/fast20/presentation/patel-hpc-systems" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-conffast-patel-blwcrt-20\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-conffast-patel-blwcrt-20/-abstract" style="display: none;">
            <p class="abstract abstract-text">Large-scale high-performance computing (HPC) applications running on supercomputers produce large amounts of data routinely and store it in files on multi-PB shared parallel storage systems. Unfortunately, storage community has a limited understanding of the access and reuse patterns of these files. This paper investigates the access and reuse patterns of I/O- intensive files on a production-scale supercomputer.</p>
          </div>
          
          
          <div id="/publication/dblp-conffast-patel-blwcrt-20/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-conffast-patel-blwcrt-20/-cite" class="tex hljs" data-filename="/publication/dblp-conffast-patel-blwcrt-20/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2020">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [FAST 2020 (b)]
            
          
        </span>
        <a href="https://www.usenix.org/conference/fast20/presentation/lu" class="article-title" 
          style="color:inherit; font-weight: normal;">Making Disk Failure Predictions SMARTer!</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-conffast-lu-lpyts-20\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://www.usenix.org/conference/fast20/presentation/lu" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-conffast-lu-lpyts-20\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-conffast-lu-lpyts-20/-abstract" style="display: none;">
            <p class="abstract abstract-text">Disk drives are one of the most commonly replaced hardware components and continue to pose challenges for accurate failure prediction. In this work, we present analysis and findings from one of the largest disk failure prediction studies covering a total of 380,000 hard drives over a period of two months across 64 sites of a large leading data center operator. Our proposed machine learning based models predict disk failures with 0.95 F-measure and 0.95 Matthews correlation coefficient (MCC) for 10-days prediction horizon on average.</p>
          </div>
          
          
          <div id="/publication/dblp-conffast-lu-lpyts-20/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-conffast-lu-lpyts-20/-cite" class="tex hljs" data-filename="/publication/dblp-conffast-lu-lpyts-20/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2020">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [FAST 2020 (c)]
            
          
        </span>
        <a href="https://www.usenix.org/conference/fast20/presentation/patel-gift" class="article-title" 
          style="color:inherit; font-weight: normal;">GIFT: A Coupon Based Throttle-and-Reward Mechanism for Fair and Efficient I/O Bandwidth Management on Parallel Storage Systems</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-conffast-patel-0-t-20\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://www.usenix.org/conference/fast20/presentation/patel-gift" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-conffast-patel-0-t-20\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-conffast-patel-0-t-20/-abstract" style="display: none;">
            <p class="abstract abstract-text">Large-scale parallel applications are highly data-intensive and perform terabytes of I/O routinely. Unfortunately, on a large-scale system where multiple applications run concurrently, I/O contention negatively affects system efficiency and causes unfair bandwidth allocation among applications. To address these challenges, this paper introduces GIFT, a principled dynamic approach to achieve fairness among competing applications and improve system efficiency.</p>
          </div>
          
          
          <div id="/publication/dblp-conffast-patel-0-t-20/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-conffast-patel-0-t-20/-cite" class="tex hljs" data-filename="/publication/dblp-conffast-patel-0-t-20/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2020">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [IPDPS 2020]
            
          
        </span>
        <a href="https://doi.org/10.1109/IPDPS47924.2020.00087" class="article-title" 
          style="color:inherit; font-weight: normal;">What does Power Consumption Behavior of HPC Jobs Reveal? : Demystifying, Quantifying, and Predicting Power Consumption Characteristics</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confipps-patel-wehzt-20\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/IPDPS47924.2020.00087" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confipps-patel-wehzt-20\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confipps-patel-wehzt-20/-abstract" style="display: none;">
            <p class="abstract abstract-text">As we approach exascale computing, large-scale HPC systems are becoming increasingly power-constrained, requiring them to run HPC workloads in an energy-efficient manner. The first step toward achieving this goal is to better understand, analyze, and quantify the power consumption characteristics of HPC jobs. However, there is a lack of understanding of the power consumption characteristics of HPC jobs which run on production HPC systems. Such characterization is required to guide the design of the next generation of power-aware resource management. To the best of our knowledge, we are the first study to open-source the data and analysis of power-consumption characteristics of HPC jobs and users from two medium-scale production HPC clusters.</p>
          </div>
          
          
          <div id="/publication/dblp-confipps-patel-wehzt-20/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confipps-patel-wehzt-20/-cite" class="tex hljs" data-filename="/publication/dblp-confipps-patel-wehzt-20/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-2 year-2020">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [SNAM 2019]
            
          
        </span>
        <a href="https://doi.org/10.1007/s13278-019-0614-6" class="article-title" 
          style="color:inherit; font-weight: normal;">Resilience and coevolution of preferential interdependent networks</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-journalssnam-ganguly-mpst-20\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1007/s13278-019-0614-6" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-journalssnam-ganguly-mpst-20\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-journalssnam-ganguly-mpst-20/-abstract" style="display: none;">
            <p class="abstract abstract-text">We propose a new model for the study of resilience of coevolving multiplex scale-free networks. Our network model, called preferential interdependent networks, is a novel continuum over scale-free networks parameterized by their correlation ρ,0≤ρ≤1. Our failure and recovery model ties the propensity of a node, both to fail and to assist in recovery, to its importance. We show, analytically, that our network model can achieve any γ,2≤γ≤3 for the exponent of the power law of the degree distribution; this is superior to existing multiplex models and allows us better fidelity in representing real-world networks. Our failure and recovery model is also a departure from the much studied cascading error model based on the giant component; it allows for surviving important nodes to send assistance to the damaged nodes to enable their recovery. This better reflects the reality of recovery in man-made networks such as social networks and infrastructure networks. Our main finding, based on simulations, is that resilient preferential interdependent networks are those in which the layers are neither completely correlated (ρ=1) nor completely uncorrelated (ρ=0) but instead semi-correlated (ρ≈0.1−0.3). This finding is consistent with the real-world experience where complex man-made networks typically bounce back quickly from stress. In an attempt to explain our intriguing empirical discovery, we present an argument for why semi-correlated multiplex networks can be the most resilient. Our argument can be seen as an explanation of plausibility or as an incomplete mathematical proof subject to certain technical conjectures that we make explicit.</p>
          </div>
          
          
          <div id="/publication/dblp-journalssnam-ganguly-mpst-20/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-journalssnam-ganguly-mpst-20/-cite" class="tex hljs" data-filename="/publication/dblp-journalssnam-ganguly-mpst-20/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-2 year-2020">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [RS 2020]
            
          
        </span>
        <a href="https://doi.org/10.3390/rs12020326" class="article-title" 
          style="color:inherit; font-weight: normal;">Comparing Performances of Five Distinct Automatic Classifiers for Fin Whale Vocalizations in Beamformed Spectrograms of Coherent Hydrophone Array</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-journalsremotesensing-garcia-cgthtr-20\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.3390/rs12020326" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-journalsremotesensing-garcia-cgthtr-20\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-journalsremotesensing-garcia-cgthtr-20/-abstract" style="display: none;">
            <p class="abstract abstract-text">A large variety of sound sources in the ocean, including biological, geophysical, and man-made, can be simultaneously monitored over instantaneous continental-shelf scale regions via the passive ocean acoustic waveguide remote sensing (POAWRS) technique by employing a large-aperture densely-populated coherent hydrophone array system. Millions of acoustic signals received on the POAWRS system per day can make it challenging to identify individual sound sources. An automated classification system is necessary to enable sound sources to be recognized. Here, the objectives are to (i) gather a large training and test data set of fin whale vocalization and other acoustic signal detections; (ii) build multiple fin whale vocalization classifiers, including a logistic regression, support vector machine (SVM), decision tree, convolutional neural network (CNN), and long short-term memory (LSTM) network; (iii) evaluate and compare performance of these classifiers using multiple metrics including accuracy, precision, recall and F1-score; and (iv) integrate one of the classifiers into the existing POAWRS array and signal processing software. The findings presented here will (1) provide an automatic classifier for near real-time fin whale vocalization detection and recognition, useful in marine mammal monitoring applications; and (2) lay the foundation for building an automatic classifier applied for near real-time detection and recognition of a wide variety of biological, geophysical, and man-made sound sources typically detected by the POAWRS system in the ocean</p>
          </div>
          
          
          <div id="/publication/dblp-journalsremotesensing-garcia-cgthtr-20/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-journalsremotesensing-garcia-cgthtr-20/-cite" class="tex hljs" data-filename="/publication/dblp-journalsremotesensing-garcia-cgthtr-20/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2020">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [HPCA 2020]
            
          
        </span>
        <a href="https://doi.org/10.1109/HPCA47549.2020.00025" class="article-title" 
          style="color:inherit; font-weight: normal;">CLITE: Efficient and QoS-Aware Co-Location of Multiple Latency-Critical Jobs for Warehouse Scale Computers</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confhpca-patel-t-20\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/HPCA47549.2020.00025" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confhpca-patel-t-20\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confhpca-patel-t-20/-abstract" style="display: none;">
            <p class="abstract abstract-text">Large-scale data centers run latency-critical jobs with quality-of-service (QoS) requirements, and throughput-oriented background jobs, which need to achieve high perfor-mance. Previous works have proposed methods which cannot co-locate multiple latency-critical jobs with multiple back-grounds jobs while: (1) meeting the QoS requirements of all latency-critical jobs, and (2) maximizing the performance of the background jobs. This paper proposes CLITE, a Bayesian Optimization-based, multi-resource partitioning technique which achieves these goals.</p>
          </div>
          
          
          <div id="/publication/dblp-confhpca-patel-t-20/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confhpca-patel-t-20/-cite" class="tex hljs" data-filename="/publication/dblp-confhpca-patel-t-20/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2019">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [DAC 2019]
            
          
        </span>
        <a href="https://doi.org/10.1145/3316781.3317931" class="article-title" 
          style="color:inherit; font-weight: normal;">What does Vibration do to Your SSD?</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confdac-bhimani-pmt-19\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1145/3316781.3317931" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confdac-bhimani-pmt-19\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confdac-bhimani-pmt-19/-abstract" style="display: none;">
            <p class="abstract abstract-text">Vibration generated in modern computing environments such as autonomous vehicles, edge computing infrastructure, and data center systems is an increasing concern. In this paper, we systematically measure, quantify and characterize the impact of vibration on the performance of SSD devices. Our experiments and analysis uncover that exposure to both short-term and long-term vibration, even within the vendor-specified limits, can significantly affect SSD I/O performance and reliability.</p>
          </div>
          
          
          <div id="/publication/dblp-confdac-bhimani-pmt-19/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confdac-bhimani-pmt-19/-cite" class="tex hljs" data-filename="/publication/dblp-confdac-bhimani-pmt-19/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2019">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [SC 2019]
            
          
        </span>
        <a href="https://doi.org/10.1145/3295500.3356183" class="article-title" 
          style="color:inherit; font-weight: normal;">Revisiting I/O behavior in large-scale storage systems: the expected and the unexpected</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confsc-patel-blt-19\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1145/3295500.3356183" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confsc-patel-blt-19\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confsc-patel-blt-19/-abstract" style="display: none;">
            <p class="abstract abstract-text">Large-scale applications typically spend a large fraction of their execution time performing I/O to a parallel storage system. However, with rapid progress in compute and storage system stack of large-scale systems, it is critical to investigate and update our understanding of the I/O behavior of large-scale applications. Toward that end, in this work, we monitor, collect and analyze a year worth of storage system data from a large-scale production parallel storage system. We perform temporal, spatial and correlative analysis of the system and uncover surprising patterns which defy existing assumptions and have important implications for future systems.</p>
          </div>
          
          
          <div id="/publication/dblp-confsc-patel-blt-19/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confsc-patel-blt-19/-cite" class="tex hljs" data-filename="/publication/dblp-confsc-patel-blt-19/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2019">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [HPDC 2019]
            
          
        </span>
        <a href="https://doi.org/10.1145/3307681.3326607" class="article-title" 
          style="color:inherit; font-weight: normal;">PERQ: Fair and Efficient Power Management of Power-Constrained Large-Scale Computing Systems</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confhpdc-patel-t-19\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1145/3307681.3326607" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confhpdc-patel-t-19\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confhpdc-patel-t-19/-abstract" style="display: none;">
            <p class="abstract abstract-text">Large-scale computing systems are becoming increasingly more power-constrained, but these systems employ hardware over- provisioning to achieve higher system throughput because applications often do not consume the peak power capacity of nodes. Unfortunately, focusing on system throughput alone can lead to severe unfairness among multiple concurrently-running applications. This paper introduces PERQ, a new feedback-based principled approach to improve system throughput while achieving fairness among concurrent applications.</p>
          </div>
          
          
          <div id="/publication/dblp-confhpdc-patel-t-19/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confhpdc-patel-t-19/-cite" class="tex hljs" data-filename="/publication/dblp-confhpdc-patel-t-19/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2019">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [DATE 2019]
            
          
        </span>
        <a href="https://doi.org/10.23919/DATE.2019.8714781" class="article-title" 
          style="color:inherit; font-weight: normal;">PCFI: Program Counter Guided Fault Injection for Accelerating GPU Reliability Assessment</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confdate-previlon-ktk-19\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.23919/DATE.2019.8714781" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confdate-previlon-ktk-19\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confdate-previlon-ktk-19/-abstract" style="display: none;">
            <p class="abstract abstract-text">Reliability has become a first-class design objective for GPU devices due to increasing soft-error rate. To assess the reliability of GPU programs, researchers rely on software fault-injection methods. Unfortunately, software fault-injection process is prohibitively expensive, requiring multiple days to complete a statistically sound fault-injection campaign. Therefore, to address this challenge, this paper proposes a novel fault-injection method, PCFI, that reduces the number of fault injections by exploiting the predictability in fault-injection outcome based on the program counter of the soft-error affected instruction. Evaluation on a variety of GPU programs covering a wide range of application domains shows that PCFI reduces the time to complete fault-injection campaigns by 22% on average, without sacrificing accuracy.</p>
          </div>
          
          
          <div id="/publication/dblp-confdate-previlon-ktk-19/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confdate-previlon-ktk-19/-cite" class="tex hljs" data-filename="/publication/dblp-confdate-previlon-ktk-19/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2019">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [ICAC 2019]
            
          
        </span>
        <a href="https://doi.org/10.1109/ICAC.2019.00027" class="article-title" 
          style="color:inherit; font-weight: normal;">Characterizing Disk Health Degradation and Proactively Protecting Against Disk Failures for Reliable Storage Systems</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-conficac-huang-lfstc-19\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/ICAC.2019.00027" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-conficac-huang-lfstc-19\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-conficac-huang-lfstc-19/-abstract" style="display: none;">
            <p class="abstract abstract-text">The booming of cloud computing, online services and big data applications have resulted in dramatic expansion of storage systems. Meanwhile, disk drives are reported to be the most commonly replaced hardware component. Disk failures cause service downtime and even data loss, costing enterprises multi-trillion dollars per year. Existing disk failure management approaches are mostly reactive and incur high overheads. To overcome these problems, in this paper, we present a proactive, cost-effective solution to managing large-scale production storage systems. We aim to uncover the entire process in which disk's health deteriorates and forecast when disk drives will fail in the future. Due to a common lack of diagnostic information of disk failures, we rely on the Self-Monitoring, Analysis and Reporting Technology (SMART) data and explore statistical analysis techniques to identify the start of disk degradation. We then model the disk degradation processes as functions of SMART attributes, which eliminates the dependency on time and thus I/O workload. Experimental results from over 23,000 enterprise-class disk drives in a production data center show that our derived models can accurately quantify the degradation of disk health, which enables us to proactively protect data against disk failures. We also investigate several types of disk failures and propose remediation mechanisms to prolong disk lifetime.</p>
          </div>
          
          
          <div id="/publication/dblp-conficac-huang-lfstc-19/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-conficac-huang-lfstc-19/-cite" class="tex hljs" data-filename="/publication/dblp-conficac-huang-lfstc-19/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-2 year-2019">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [MTAGS 2017]
            
          
        </span>
        <a href="http://arxiv.org/abs/1905.09166" class="article-title" 
          style="color:inherit; font-weight: normal;">Two stage cluster for resource optimization with Apache Mesos</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-journalscorrabs-1905-09166\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="http://arxiv.org/abs/1905.09166" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-journalscorrabs-1905-09166\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-journalscorrabs-1905-09166/-abstract" style="display: none;">
            <p class="abstract abstract-text">As resource estimation for jobs is difficult, users often overestimate their requirements. Both commercial clouds and academic campus clusters suffer from low resource utilization and long wait times as the resource estimates for jobs, provided by users, is inaccurate. We present an approach to statistically estimate the actual resource requirement of a job in a Little cluster before the run in a Big cluster. The initial estimation on the little cluster gives us a view of how much actual resources a job requires. This initial estimate allows us to accurately allocate resources for the pending jobs in the queue and thereby improve throughput and resource utilization. In our experiments, we determined resource utilization estimates with an average accuracy of 90% for memory and 94% for CPU, while we make better utilization of memory by an average of 22% and CPU by 53%, compared to the default job submission methods on Apache Aurora and Apache Mesos. </p>
          </div>
          
          
          <div id="/publication/dblp-journalscorrabs-1905-09166/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-journalscorrabs-1905-09166/-cite" class="tex hljs" data-filename="/publication/dblp-journalscorrabs-1905-09166/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2019">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [CCGRID 2019]
            
          
        </span>
        <a href="https://doi.org/10.1109/CCGRID.2019.00033" class="article-title" 
          style="color:inherit; font-weight: normal;">Towards Enabling Dynamic Resource Estimation and Correction for Improving Utilization in an Apache Mesos Cloud Environment</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confccgrid-rattihalli-gt-19\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/CCGRID.2019.00033" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confccgrid-rattihalli-gt-19\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confccgrid-rattihalli-gt-19/-abstract" style="display: none;">
            <p class="abstract abstract-text">Academic cloud infrastructures require users to specify an estimate of their resource requirements. The resource usage for applications often depends on the input file sizes, parameters, optimization flags, and attributes, specified for each run. Incorrect estimation can result in low resource utilization of the entire infrastructure and long wait times for jobs in the queue. We have designed a Resource Utilization based Migration (RUMIG) system to address the resource estimation problem. We present the overall architecture of the two-stage elastic cluster design, the Apache Mesos-specific container migration system, and analyze the performance for several scientific workloads on three different cloud/cluster environments. In this paper we (b) present a design and implementation for container migration in a Mesos environment, (c) evaluate the effect of right-sizing and cluster elasticity on overall performance, (d) analyze different profiling intervals to determine the best fit, (e) determine the overhead of our profiling mechanism. Compared to the default use of Apache Mesos, in the best cases, RUMIG provides a gain of 65% in runtime (local cluster), 51% in CPU utilization in the Chameleon cloud, and 27% in memory utilization in the Jetstream cloud.</p>
          </div>
          
          
          <div id="/publication/dblp-confccgrid-rattihalli-gt-19/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confccgrid-rattihalli-gt-19/-cite" class="tex hljs" data-filename="/publication/dblp-confccgrid-rattihalli-gt-19/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2019">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [CLOUD 2019]
            
          
        </span>
        <a href="https://doi.org/10.1109/CLOUD.2019.00018" class="article-title" 
          style="color:inherit; font-weight: normal;">Exploring Potential for Non-Disruptive Vertical Auto Scaling and Resource Estimation in Kubernetes</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confiee-ecloud-rattihalli-glt-19\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/CLOUD.2019.00018" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confiee-ecloud-rattihalli-glt-19\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confiee-ecloud-rattihalli-glt-19/-abstract" style="display: none;">
            <p class="abstract abstract-text">Cloud platforms typically require users to provide resource requirements for applications so that resource managers can schedule containers with adequate allocations. However, the requirements for container resources often depend on numerous factors such as application input parameters, optimization flags, input files, and attributes that are specified for each run. So, it is complex for users to estimate the resource requirements for a given container accurately, leading to resource over-estimation that negatively affects overall utilization. We have designed a Resource Utilization Based Autoscaling System (RUBAS) that can dynamically adjust the allocation of containers running in a Kubernetes cluster. RUBAS improves upon the Kubernetes Vertical Pod Autoscaler (VPA) system non-disruptively by incorporating container migration. Our experiments use multiple scientific benchmarks. We analyze the allocation pattern of RUBAS with Kubernetes VPA. We compare the performance of container migration for in-place and remote node migration and we evaluate the overhead in RUBAS. Our results show that compared to Kubernetes VPA, RUBAS improves the CPU and memory utilization of the cluster by 10% and reduces the runtime by 15% with an overhead for each application ranging from 5% to 20%.</p>
          </div>
          
          
          <div id="/publication/dblp-confiee-ecloud-rattihalli-glt-19/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confiee-ecloud-rattihalli-glt-19/-cite" class="tex hljs" data-filename="/publication/dblp-confiee-ecloud-rattihalli-glt-19/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-2 year-2019">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [TPDS 2019]
            
          
        </span>
        <a href="https://doi.org/10.1109/TPDS.2018.2865471" class="article-title" 
          style="color:inherit; font-weight: normal;">An Analysis Workflow-Aware Storage System for Multi-Core Active Flash Arrays</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-journalstpds-sim-vkvtb-19\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/TPDS.2018.2865471" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-journalstpds-sim-vkvtb-19\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-journalstpds-sim-vkvtb-19/-abstract" style="display: none;">
            <p class="abstract abstract-text">The need for novel data analysis is urgent in the face of a data deluge from modern applications. Traditional approaches to data analysis incur significant data movement costs, moving data back and forth between the storage system and the processor. Emerging Active Flash devices enable processing on the flash, where the data already resides. An array of such Active Flash devices allows us to revisit how analysis workflows interact with storage systems. By seamlessly blending together the flash storage and data analysis, we create an analysis workflow-aware storage system, AnalyzeThis. Our guiding principle is that analysis-awareness be deeply ingrained in each and every layer of the storage, elevating data analyses as first-class citizens, and transforming AnalyzeThis into a potent analytics-aware appliance. To evaluate the AnalyzeThis system, we have adopted both emulation and simulation approaches. In particular, we have evaluated AnalyzeThis by implementing the AnalyzeThis storage system on top of the Active Flash Array's emulation platform. We have also implemented an event-driven AnalyzeThis simulator, called AnalyzeThisSim, which allows us to address the limitations of the emulation platform, e.g., performance impact of using multi-core SSDs. The results from our emulation and simulation platforms indicate that AnalyzeThis is a viable approach for expediting workflow execution and minimizing data movement.</p>
          </div>
          
          
          <div id="/publication/dblp-journalstpds-sim-vkvtb-19/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-journalstpds-sim-vkvtb-19/-cite" class="tex hljs" data-filename="/publication/dblp-journalstpds-sim-vkvtb-19/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2018">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [DSN 2018 (a)]
            
          
        </span>
        <a href="https://doi.org/10.1109/DSN.2018.00022" class="article-title" 
          style="color:inherit; font-weight: normal;">Machine Learning Models for GPU Error Prediction in a Large Scale HPC System</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confdsn-nie-xgpest-18\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/DSN.2018.00022" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confdsn-nie-xgpest-18\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confdsn-nie-xgpest-18/-abstract" style="display: none;">
            <p class="abstract abstract-text">GPUs are widely deployed on large-scale HPC systems to provide powerful computational capability for scientific applications from various domains. As those applications are normally long-running, investigating the characteristics of GPU errors becomes imperative for reliability. In this paper, we first study the system conditions that trigger GPU errors using six-month trace data collected from a large-scale, operational HPC system. Then, we use machine learning to predict the occurrence of GPU errors, by taking advantage of temporal and spatial dependencies of the trace data. The resulting machine learning prediction framework is robust and accurate under different workloads.</p>
          </div>
          
          
          <div id="/publication/dblp-confdsn-nie-xgpest-18/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confdsn-nie-xgpest-18/-cite" class="tex hljs" data-filename="/publication/dblp-confdsn-nie-xgpest-18/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2018">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [DSN 2018 (b)]
            
          
        </span>
        <a href="https://doi.org/10.1109/DSN.2018.00021" class="article-title" 
          style="color:inherit; font-weight: normal;">Shiraz: Exploiting System Reliability and Application Resilience Characteristics to Improve Large Scale System Throughput</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confdsn-garg-pct-18\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/DSN.2018.00021" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confdsn-garg-pct-18\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confdsn-garg-pct-18/-abstract" style="display: none;">
            <p class="abstract abstract-text">Large-scale applications rely on resilience mechanisms such as checkpoint-restart to make forward progress in the presence of failures. Unfortunately, this incurs huge I/O overhead and impedes productivity. To mitigate this challenge, this paper introduces a new technique, Shiraz, which demonstrates how to exploit differences in the checkpointing overhead among applications and knowledge of temporal characteristics of failures to improve both the overall system throughput and performance of individual applications.</p>
          </div>
          
          
          <div id="/publication/dblp-confdsn-garg-pct-18/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confdsn-garg-pct-18/-cite" class="tex hljs" data-filename="/publication/dblp-confdsn-garg-pct-18/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2018">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [DSN 2018 (c)]
            
          
        </span>
        <a href="https://doi.org/10.1109/DSN.2018.00023" class="article-title" 
          style="color:inherit; font-weight: normal;">Understanding and Analyzing Interconnect Errors and Network Congestion on a Large Scale HPC System</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confdsn-kumar-gpwsfet-18\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/DSN.2018.00023" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confdsn-kumar-gpwsfet-18\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confdsn-kumar-gpwsfet-18/-abstract" style="display: none;">
            <p class="abstract abstract-text">Today's High Performance Computing (HPC) systems are capable of delivering performance in the order of petaflops due to the fast computing devices, network interconnect, and back-end storage systems. In particular, interconnect resilience and congestion resolution methods have a major impact on the overall interconnect and application performance. This is especially true for scientific applications running multiple processes on different compute nodes as they rely on fast network messages to communicate and synchronize frequently. Unfortunately, the HPC community lacks state-of-practice experience reports that detail how different interconnect errors and congestion events occur on large-scale HPC systems. Therefore, in this paper, we process and analyze interconnect data of the Titan supercomputer to develop a thorough understanding of interconnects faults, errors and congestion events. We also study the interaction between interconnect, errors, network congestion and application characteristics.</p>
          </div>
          
          
          <div id="/publication/dblp-confdsn-kumar-gpwsfet-18/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confdsn-kumar-gpwsfet-18/-cite" class="tex hljs" data-filename="/publication/dblp-confdsn-kumar-gpwsfet-18/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2018">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              [BigData 2018]
            
          
        </span>
        <a href="https://doi.org/10.1109/BigData.2018.8622643" class="article-title" 
          style="color:inherit; font-weight: normal;">Reliability Characterization of Solid State Drives in a Scalable Production Datacenter</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confbigdataconf-liang-qhhfstcsm-18\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/BigData.2018.8622643" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confbigdataconf-liang-qhhfstcsm-18\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confbigdataconf-liang-qhhfstcsm-18/-abstract" style="display: none;">
            <p class="abstract abstract-text">In recent years, NAND flash-based solid state drives (SSD) have been widely used in datacenters due to their better performance compared with the traditional hard disk drives. However, little is known about the reliability characteristics of SSDs in production systems. Existing works study the statistical distributions of SSD failures in the field. However, they do not go deep into SSD drives and investigate the unique error types and health dynamics that distinguish SSDs from hard disk drives. In this paper, we explore the SSD-specific SMART (Self-Monitoring, Analysis, and Reporting Technology) attributes to conduct an in-depth analysis of SSD reliability in a production environment. Data is collected from a scalable production system having several physical locations. Our dataset contains over a million records with more than twenty attributes. We leverage machine learning technologies, specifically data clustering and correlation analysis methods, to discover groups of SSDs which have different health status and relations among SSD-specific SMART attributes. Our results show that 1) Media wear affects the reliability of SSDs more than any other factors, and 2) SSDs transit from one health group to another which infers the reliability degradation of those drives. To the best of our knowledge, this is the first study that investigates SSD-specific SMART data to characterize SSD reliability in a production environment.</p>
          </div>
          
          
          <div id="/publication/dblp-confbigdataconf-liang-qhhfstcsm-18/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confbigdataconf-liang-qhhfstcsm-18/-cite" class="tex hljs" data-filename="/publication/dblp-confbigdataconf-liang-qhhfstcsm-18/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2018">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [ACM 2018]
            
          
        </span>
        <a href="https://doi.org/10.1109/ASONAM.2018.8508541" class="article-title" 
          style="color:inherit; font-weight: normal;">Resilience and the Coevolution of Interdependent Multiplex Networks</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confasunam-ganguly-mst-18\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/ASONAM.2018.8508541" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confasunam-ganguly-mst-18\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confasunam-ganguly-mst-18/-abstract" style="display: none;">
            <p class="abstract abstract-text">We propose a new model for the study of resilience of coevolving multiplex scale-free networks. Our network model, called preferential interdependent networks, is a novel continuum over scale-free networks parameterized by their correlation p, 0 ≤ p ≤1. Our failure and recovery model ties the propensity of a node, both to fail and to assist in recovery, to its importance. We show, analytically, that our network model can achieve any γ, 2 ≤ γ ≤ 3 for the exponent of the power law of the degree distribution; this is superior to existing multiplex models and allows us better fidelity in representing real-world networks. Our failure and recovery model is also a departure from the much studied cascading error model based on the giant component; it allows for surviving important nodes to send assistance to the damaged nodes to enable their recovery. This better reflects the reality of recovery in man-made networks such as social networks and infrastructure networks. Our main finding, based on simulations, is that resilient preferential interdependent networks are those in which the layers are neither completely correlated (p = 1) nor completely uncorrelated (p= 0) but instead semi-correlated (p ≈ 0.1 - 0.3). This finding is consistent with the real-world experience where complex man-made networks typically bounce back quickly from stress. In an attempt to explain our intriguing empirical discovery we present an argument for why semi-correlated multiplex networks can be the most resilient. Our argument can be seen as an explanation of plausibility or as an incomplete mathematical proof subject to certain technical conjectures that we make explicit.</p>
          </div>
          
          
          <div id="/publication/dblp-confasunam-ganguly-mst-18/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confasunam-ganguly-mst-18/-cite" class="tex hljs" data-filename="/publication/dblp-confasunam-ganguly-mst-18/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2018">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [ICCCN 2018]
            
          
        </span>
        <a href="https://doi.org/10.1109/ICCCN.2018.8487322" class="article-title" 
          style="color:inherit; font-weight: normal;">Exploring the Optimal Platform Configuration for Power-Constrained HPC Workflows</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-conficccn-tang-hgvt-18\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/ICCCN.2018.8487322" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-conficccn-tang-hgvt-18\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-conficccn-tang-hgvt-18/-abstract" style="display: none;">
            <p class="abstract abstract-text">In high-performance computing (HPC) workflows, data analytics is typically utilized to gain insights from scientific simulations. Approaching the era of exascale, online analysis is gaining popularity due to the savings of I/O to persistent storage. As computing capability keeps growing, power consumption is becoming critical to HPC facilities. Enforcing power limits is emerging as a practical trend for power-constrained HPC facilities. However, it remains unclear how to choose the appropriate power limits for various HPC workflows and how to distribute the power limit of a workflow between simulation and analysis. In addition, given a power limit, it is unclear what the optimal scales and power capping levels are for various workflows, especially when taking reliability into account. In order to resolve these issues in power-constrained HPC, in this paper, we propose a reliability-aware model to determine the aforementioned platform configurations for HPC workflows. We also validate our model and present model-driven studies for a wide range of real-system scenarios. Our study reveals interesting insights about how platform configuration affects the performance and energy efficiency of HPC workflows under power constraints.</p>
          </div>
          
          
          <div id="/publication/dblp-conficccn-tang-hgvt-18/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-conficccn-tang-hgvt-18/-cite" class="tex hljs" data-filename="/publication/dblp-conficccn-tang-hgvt-18/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2017">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [SC 2017 (a)]
            
          
        </span>
        <a href="https://doi.org/10.1145/3126908.3126946" class="article-title" 
          style="color:inherit; font-weight: normal;">GUIDE: a scalable information directory service to collect, federate, and analyze logs for operational insights into a leadership HPC facility</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confsc-vazhkudai-mtzwog-17\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1145/3126908.3126946" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confsc-vazhkudai-mtzwog-17\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confsc-vazhkudai-mtzwog-17/-abstract" style="display: none;">
            <p class="abstract abstract-text">In this paper, we describe the GUIDE framework used to collect, federate, and analyze log data from the Oak Ridge Leadership Computing Facility (OLCF), and how we use that data to derive insights into facility operations. We collect system logs and extract monitoring data at every level of the various OLCF subsystems, and have developed a suite of pre-processing tools to make the raw data consumable. The cleansed logs are then ingested and federated into a central, scalable data warehouse, Splunk, that offers storage, indexing, querying, and visualization capabilities. We have further developed and deployed a set of tools to analyze these multiple disparate log streams in concert and derive operational insights. We describe our experience from developing and deploying the GUIDE infrastructure, and deriving valuable insights on the various subsystems, based on two years of operations in the production OLCF environment.</p>
          </div>
          
          
          <div id="/publication/dblp-confsc-vazhkudai-mtzwog-17/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confsc-vazhkudai-mtzwog-17/-cite" class="tex hljs" data-filename="/publication/dblp-confsc-vazhkudai-mtzwog-17/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2017">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [SC 2017 (b)]
            
          
        </span>
        <a href="https://doi.org/10.1145/3126908.3126937" class="article-title" 
          style="color:inherit; font-weight: normal;">Failures in large scale systems: long-term measurement, analysis, and implications</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confsc-gupta-pet-17\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1145/3126908.3126937" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confsc-gupta-pet-17\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confsc-gupta-pet-17/-abstract" style="display: none;">
            <p class="abstract abstract-text">Resilience is one of the key challenges in maintaining high efficiency of future extreme scale supercomputers. Researchers and system practitioners rely on field-data studies to understand reliability characteristics and plan for future HPC systems. In this work, we compare and contrast the reliability characteristics of multiple large-scale HPC production systems. Our study covers more than one billion compute node hours across five different systems over a period of 8 years. We confirm previous findings which continue to be valid, discover new findings, and discuss their implications.</p>
          </div>
          
          
          <div id="/publication/dblp-confsc-gupta-pet-17/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confsc-gupta-pet-17/-cite" class="tex hljs" data-filename="/publication/dblp-confsc-gupta-pet-17/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2017">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [MASCOTS 2017 (a)]
            
          
        </span>
        <a href="https://doi.org/10.1109/MASCOTS.2017.35" class="article-title" 
          style="color:inherit; font-weight: normal;">Toward Managing HPC Burst Buffers Effectively: Draining Strategy to Regulate Bursty I/O Behavior</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confmascots-tang-hhlvt-17\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/MASCOTS.2017.35" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confmascots-tang-hhlvt-17\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confmascots-tang-hhlvt-17/-abstract" style="display: none;">
            <p class="abstract abstract-text">HPC (high-performance computing) applications usually show bursty I/O behaviors. In order to expedite the applications, permanent storage systems are usually provisioned to serve such I/O bursts. Approaching the era of exascale computing, non-volatile RAM is introduced as burst buffers, to absorb the bursty bulk data and relax the I/O provisioning requirement of the permanent storage systems. However, without judiciously draining the burst buffers, I/O bursts are passed down to the underlying storage systems, which causes severe I/O contention issues.In order to minimize the I/O provisioning requirement and resolve the issues caused by I/O bursts, we propose a proactive draining scheme to manage the draining process of distributed node-local burst buffers. In addition, we develop an I/O provisioning model to predict the minimized I/O provisioning requirement for permanent storage systems. Evaluation results show that applying the proactive draining scheme largely relaxes the I/O provisioning requirement while preserving the I/O performance of underlying storage systems.</p>
          </div>
          
          
          <div id="/publication/dblp-confmascots-tang-hhlvt-17/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confmascots-tang-hhlvt-17/-cite" class="tex hljs" data-filename="/publication/dblp-confmascots-tang-hhlvt-17/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2017">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [MASCOTS 2017 (b)]
            
          
        </span>
        <a href="https://doi.org/10.1109/MASCOTS.2017.12" class="article-title" 
          style="color:inherit; font-weight: normal;">Characterizing Temperature, Power, and Soft-Error Behaviors in Data Center Systems: Insights, Challenges, and Opportunities</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confmascots-nie-xgest-17\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/MASCOTS.2017.12" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confmascots-nie-xgest-17\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confmascots-nie-xgest-17/-abstract" style="display: none;">
            <p class="abstract abstract-text">GPUs have become part of the mainstream high performance computing facilities that increasingly require more computational power to simulate physical phenomena quickly and accurately. However, GPU nodes also consume significantly more power than traditional CPU nodes, and high power consumption introduces new system operation challenges, including increased temperature, power/cooling cost, and lower system reliability. This paper explores how power consumption and temperature characteristics affect reliability, provides insights into what are the implications of such understanding, and how to exploit these insights toward predicting GPU errors using neural networks.</p>
          </div>
          
          
          <div id="/publication/dblp-confmascots-nie-xgest-17/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confmascots-nie-xgest-17/-cite" class="tex hljs" data-filename="/publication/dblp-confmascots-nie-xgest-17/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-2 year-2017">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [TOMPECS 2017]
            
          
        </span>
        <a href="https://doi.org/10.1145/3055280" class="article-title" 
          style="color:inherit; font-weight: normal;">Obtaining and Managing Answer Quality for Online Data-Intensive Services</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-journalstompecs-kelley-smthe-17\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1145/3055280" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-journalstompecs-kelley-smthe-17\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-journalstompecs-kelley-smthe-17/-abstract" style="display: none;">
            <p class="abstract abstract-text">Online data-intensive (OLDI) services use anytime algorithms to compute over large amounts of data and respond quickly. Interactive response times are a priority, so OLDI services parallelize query execution across distributed software components and return best effort answers based on the data so far processed. Omitted data from slow components could lead to better answers, but tracing online how much better the answers could be is difficult. We propose Ubora, a design approach to measure the effect of slow-running components on the quality of answers. Ubora randomly samples online queries and executes them a second time. The first online execution omits data from slow components and provides interactive answers. The second execution uses mature results from intermediate components completed after the online execution finishes. Ubora uses memoization to speed up mature executions by replaying network messages exchanged between components. Our systems-level implementation works for a wide range of services, including Hadoop/Yarn, Apache Lucene, the EasyRec Recommendation Engine, and the OpenEphyra question-answering system. Ubora computes answer quality with more mature executions per second than competing approaches that do not use memoization. With Ubora, we show that answer quality is effective at guiding online admission control. While achieving the same answer quality on high-priority queries, our adaptive controller had 55% higher peak throughput on low-priority queries than a competing controller guided by the rate of timeouts.</p>
          </div>
          
          
          <div id="/publication/dblp-journalstompecs-kelley-smthe-17/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-journalstompecs-kelley-smthe-17/-cite" class="tex hljs" data-filename="/publication/dblp-journalstompecs-kelley-smthe-17/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2017">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [CLUSTER 2017]
            
          
        </span>
        <a href="https://doi.org/10.1109/CLUSTER.2017.22" class="article-title" 
          style="color:inherit; font-weight: normal;">Effective Running of End-to-End HPC Workflows on Emerging Heterogeneous Architectures</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confcluster-tang-tgvh-17\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/CLUSTER.2017.22" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confcluster-tang-tgvh-17\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confcluster-tang-tgvh-17/-abstract" style="display: none;">
            <p class="abstract abstract-text">In high-performance computing (HPC), end-to-end workflows are typically utilized to gain insights from scientific simulations. An end-to-end workflow consists of scientific simulation and data analysis, and can be executed in-situ, in-transit, and offline. Existing studies on end-to-end workflows have largely focused on the high-performance execution approaches. However, the emerging heterogeneous architectures and energy concerns lead to the rethinking of workflow execution approaches. As a guide to the rethinking, this paper evaluates how to run end-to-end HPC workflows efficiently in terms of performance, energy, and error resilience. The evaluation covers emerging heterogeneous processor architectures, processor power capping techniques, and heterogeneous-reliability memory.</p>
          </div>
          
          
          <div id="/publication/dblp-confcluster-tang-tgvh-17/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confcluster-tang-tgvh-17/-cite" class="tex hljs" data-filename="/publication/dblp-confcluster-tang-tgvh-17/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-2 year-2017">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [TECS 2017]
            
          
        </span>
        <a href="https://doi.org/10.1145/2930667" class="article-title" 
          style="color:inherit; font-weight: normal;">Compiler-Directed Soft Error Detection and Recovery to Avoid DUE and SDC via Tail-DMR</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-journalstecs-liu-jlt-17\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1145/2930667" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-journalstecs-liu-jlt-17\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-journalstecs-liu-jlt-17/-abstract" style="display: none;">
            <p class="abstract abstract-text">This article presents Clover, a compiler-directed soft error detection and recovery scheme for lightweight soft error resilience. The compiler carefully generates soft-error-tolerant code based on idempotent processing without explicit checkpoints. During program execution, Clover relies on a small number of acoustic wave detectors deployed in the processor to identify soft errors by sensing the wave made by a particle strike. To cope with DUEs (detected unrecoverable errors) caused by the sensing latency of error detection, Clover leverages a novel selective instruction duplication technique called tail-DMR (dual modular redundancy) that provides a region-level error containment. Once a soft error is detected by either the sensors or the tail-DMR, Clover takes care of the error as in the case of exception handling. To recover from the error, Clover simply redirects program control to the beginning of the code region where the error is detected. The experimental results demonstrate that the average runtime overhead is only 26%, which is a 75% reduction compared to that of the state-of-the-art soft error resilience technique. In addition, this article evaluates an alternative technique called tail-wait, comparing it to Clover. According to the evaluation with the different processor configurations and the various error detection latencies, Clover turns out to be a superior technique, achieving 1.06 to 3.49 × speedup over the tail-wait.</p>
          </div>
          
          
          <div id="/publication/dblp-journalstecs-liu-jlt-17/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-journalstecs-liu-jlt-17/-cite" class="tex hljs" data-filename="/publication/dblp-journalstecs-liu-jlt-17/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2017">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [MWSCAS 2017]
            
          
        </span>
        <a href="https://doi.org/10.1109/MWSCAS.2017.8053069" class="article-title" 
          style="color:inherit; font-weight: normal;">Combining architectural fault-injection and neutron beam testing approaches toward better understanding of GPU soft-error resilience</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confmwscas-previlon-etrk-17\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/MWSCAS.2017.8053069" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confmwscas-previlon-etrk-17\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confmwscas-previlon-etrk-17/-abstract" style="display: none;">
            <p class="abstract abstract-text">Transient faults continue to be a critical concern in a range of computing domains including: High-Performance Computing (HPC), scientific computing, and the automotive industry. While radiation-induced faults have been well studied and understood in microprocessors, their impact on computations on Graphic Processing Units (GPU) has received less attention. GPUs are now being used in a large number of HPC and automotive markets. Mitigating the effects of transient faults requires a thorough understanding of the interaction between applications, system software, and the underlying hardware. Developing this understanding is quite challenging mainly due to our limited ability to capture and study cross-layer reliability interactions. In this paper, we consider the combination of neutron beam testing experiments with architectural fault injection experiments to gain a deeper understanding of the relationship between the vulnerability of GPUs and the underlying workload characteristics of applications targeted for GPU devices.</p>
          </div>
          
          
          <div id="/publication/dblp-confmwscas-previlon-etrk-17/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confmwscas-previlon-etrk-17/-cite" class="tex hljs" data-filename="/publication/dblp-confmwscas-previlon-etrk-17/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2016">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [SC 2016 (a)]
            
          
        </span>
        <a href="https://doi.org/10.1109/SC.2016.41" class="article-title" 
          style="color:inherit; font-weight: normal;">Granularity and the cost of error recovery in resilient AMR scientific applications</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confsc-dubey-fgct-16\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/SC.2016.41" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confsc-dubey-fgct-16\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confsc-dubey-fgct-16/-abstract" style="display: none;">
            <p class="abstract abstract-text">Supercomputing platforms are expected to have larger failure rates in the future because of scaling and power concerns. The memory and performance impact may vary with error types and failure modes. Therefore, localized recovery schemes will be important for scientific computations, including failure modes where application intervention is suitable for recovery. We present a resiliency methodology for applications using structured adaptive mesh refinement, where failure modes map to granularities within the application for detection and correction. This approach also enables parameterization of cost for differentiated recovery. The cost model is built with tuning parameters that can be used to customize the strategy for different failure rates in different computing environments. We also show that this approach can make recovery cost proportional to the failure rate.</p>
          </div>
          
          
          <div id="/publication/dblp-confsc-dubey-fgct-16/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confsc-dubey-fgct-16/-cite" class="tex hljs" data-filename="/publication/dblp-confsc-dubey-fgct-16/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2016">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [SC 2016 (b)]
            
          
        </span>
        <a href="https://doi.org/10.1109/SC.2016.19" class="article-title" 
          style="color:inherit; font-weight: normal;">Compiler-directed lightweight checkpointing for fine-grained guaranteed soft error recovery</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confsc-liu-jlt-16\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/SC.2016.19" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confsc-liu-jlt-16\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confsc-liu-jlt-16/-abstract" style="display: none;">
            <p class="abstract abstract-text">This paper presents Bolt, a compiler-directed soft error recovery scheme, that provides fine-grained and guaranteed recovery without excessive performance and hardware overhead. To get rid of expensive hardware support, the compiler protects the architectural inputs during their entire liveness period by safely checkpointing the last updated value in idempotent regions. To minimize the performance overhead, Bolt leverages a novel compiler analysis that eliminates those checkpoints whose value can be reconstructed by other checkpointed values without compromising the recovery guarantee. As a result, Bolt incurs only 4.7% performance overhead on average which is 57% reduction compared to the state-of-the-art scheme that requires expensive hardware support for the same recovery guarantee as Bolt.</p>
          </div>
          
          
          <div id="/publication/dblp-confsc-liu-jlt-16/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confsc-liu-jlt-16/-cite" class="tex hljs" data-filename="/publication/dblp-confsc-liu-jlt-16/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2016">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [HPCA 2016]
            
          
        </span>
        <a href="https://doi.org/10.1109/HPCA.2016.7446091" class="article-title" 
          style="color:inherit; font-weight: normal;">A large-scale study of soft-errors on GPUs in the field</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confhpca-nie-tgsr-16\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/HPCA.2016.7446091" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confhpca-nie-tgsr-16\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confhpca-nie-tgsr-16/-abstract" style="display: none;">
            <p class="abstract abstract-text">Parallelism provided by the GPU architecture has enabled domain scientists to simulate physical phenomena at a much faster rate and finer granularity than what was previously possible by CPU-based large-scale clusters. Architecture researchers have been investigating reliability characteristics of GPUs and innovating techniques to increase the reliability of these emerging computing devices. Such efforts are often guided by technology projections and simplistic scientific kernels, and performed using architectural simulators and modeling tools. Lack of large-scale field data impedes the effectiveness of such efforts. This study attempts to bridge this gap by presenting a large-scale field data analysis of GPU reliability. We characterize and quantify different kinds of soft-errors on the Titan supercomputer's GPU nodes. Our study uncovers several interesting and previously unknown insights about the characteristics and impact of soft-errors.</p>
          </div>
          
          
          <div id="/publication/dblp-confhpca-nie-tgsr-16/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confhpca-nie-tgsr-16/-cite" class="tex hljs" data-filename="/publication/dblp-confhpca-nie-tgsr-16/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2016">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [IPDPS 2016]
            
          
        </span>
        <a href="https://doi.org/10.1109/IPDPS.2016.100" class="article-title" 
          style="color:inherit; font-weight: normal;">Reducing Waste in Extreme Scale Systems through Introspective Analysis</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confipps-bautista-gomez-g-16\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/IPDPS.2016.100" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confipps-bautista-gomez-g-16\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confipps-bautista-gomez-g-16/-abstract" style="display: none;">
            <p class="abstract abstract-text">Resilience is an important challenge for extreme-scale supercomputers. Today, failures in supercomputers are assumed to be uniformly distributed in time. However, recent studies show that failures in high-performance computing systems are partially correlated in time, generating periods of higher failure density. Our study of the failure logs of multiple supercomputers show that periods of higher failure density occur with up to three times more than the average. We design a monitoring system that listens to hardware events and forwards important events to the runtime to detect those regime changes. We implement a runtime capable of receiving notifications and adapt dynamically. In addition, we build an analytical model to predict the gains that such dynamic approach could achieve. We demonstrate that in some systems, our approach can reduce the wasted time by over 30%.</p>
          </div>
          
          
          <div id="/publication/dblp-confipps-bautista-gomez-g-16/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confipps-bautista-gomez-g-16/-cite" class="tex hljs" data-filename="/publication/dblp-confipps-bautista-gomez-g-16/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2016">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [DSN 2016]
            
          
        </span>
        <a href="https://doi.org/10.1109/DSN.2016.36" class="article-title" 
          style="color:inherit; font-weight: normal;">Power-Capping Aware Checkpointing: On the Interplay Among Power-Capping, Temperature, Reliability, Performance, and Energy</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confdsn-tang-tghleh-16\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/DSN.2016.36" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confdsn-tang-tghleh-16\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confdsn-tang-tghleh-16/-abstract" style="display: none;">
            <p class="abstract abstract-text">Checkpoint and restart mechanisms have been widely used in large scientific simulation applications to make forward progress in case of failures. However, none of the prior works have considered the interaction of power-constraint with temperature, reliability, performance, and checkpointing interval. It is not clear how power-capping may affect optimal checkpointing interval. What are the involved reliability, performance, and energy trade-offs? In this paper, we develop a deep understanding about the interaction between power-capping and scientific applications using checkpoint/restart as resilience mechanism, and propose a new model for the optimal checkpointing interval (OCI) under power-capping. Our study reveals several interesting, and previously unknown, insights about how power-capping affects the reliability, energy consumption, performance.</p>
          </div>
          
          
          <div id="/publication/dblp-confdsn-tang-tghleh-16/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confdsn-tang-tghleh-16/-cite" class="tex hljs" data-filename="/publication/dblp-confdsn-tang-tghleh-16/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2016">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [MICRO 2016]
            
          
        </span>
        <a href="https://doi.org/10.1109/MICRO.2016.7783728" class="article-title" 
          style="color:inherit; font-weight: normal;">Low-cost soft error resilience with unified data verification and fine-grained recovery for acoustic sensor based detection</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confmicro-liu-jlt-16\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/MICRO.2016.7783728" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confmicro-liu-jlt-16\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confmicro-liu-jlt-16/-abstract" style="display: none;">
            <p class="abstract abstract-text">This paper presents Turnstile, a hardware/software cooperative technique for low-cost soft error resilience. Leveraging the recent advance of acoustic sensor based soft error detection, Turnstile achieves guaranteed recovery by taking into account the bounded detection latency. The compiler forms verifiable regions and selectively inserts store instructions to checkpoint their register inputs so that Turnstile can verify the register/memory states with regard to a region boundary in a unified way without expensive register file protection. At runtime, for each region, Turnstile regards any stores (to both memory and register checkpoints) as unverified, and thus holds them in a store queue until the region ends and spends the time of the error detection latency. If no error is detected during the time, the verified stores are merged into memory systems, and registers are checkpointed. When all the stores including checkpointing stores prior to a region boundary are verified, the architectural and memory states with regard to the boundary are verified, thus it can serve as a recovery point. In this way, Turnstile contains the errors within the core without extra memory buffering. When an error is detected, Turnstile invalidates unverified entries in the store queue and restores the checkpointed register values to get the architectural and memory states back to what they were at the most recently verified region boundary. Then, Turnstile simply redirects program control to the verified region boundary and continues execution. The experimental results demonstrate that Turnstile can offer guaranteed soft error recovery with low performance overhead (<;8% on average).   </p>
          </div>
          
          
          <div id="/publication/dblp-confmicro-liu-jlt-16/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confmicro-liu-jlt-16/-cite" class="tex hljs" data-filename="/publication/dblp-confmicro-liu-jlt-16/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-2 year-2016">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [JPDC 2016]
            
          
        </span>
        <a href="https://doi.org/10.1016/j.jpdc.2015.09.003" class="article-title" 
          style="color:inherit; font-weight: normal;">Application configuration selection for energy-efficient execution on multicore systems</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-journalsjpdc-wang-lst-16\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1016/j.jpdc.2015.09.003" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-journalsjpdc-wang-lst-16\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-journalsjpdc-wang-lst-16/-abstract" style="display: none;">
            <p class="abstract abstract-text">Modern computer systems are designed to balance performance and energy consumption. Several run-time factors, such as concurrency levels, thread mapping strategies, and dynamic voltage and frequency scaling (DVFS) should be considered in order to achieve optimal energy efficiency for a workload. Selecting appropriate run-time factors, however, is one of the most challenging tasks because the run-time factors are architecture-specific and workload-specific. While most existing works concentrate on either static analysis of the workload or run-time prediction results, in this paper, we present a hybrid two-step method that utilizes concurrency levels and DVFS settings to achieve the energy efficiency configuration for a workload. The experimental results based on a Xeon E5620 server with NPB and PARSEC benchmark suites show that the model is able to predict the energy efficient configuration accurately. On average, an additional EDP (Energy Delay Product) saving is obtained by using run-time DVFS for the entire system. An off-line optimal solution is used to compare with the proposed scheme. The experimental results show that the average extra EDP saved by the optimal solution is within 5% on selective parallel benchmarks</p>
          </div>
          
          
          <div id="/publication/dblp-journalsjpdc-wang-lst-16/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-journalsjpdc-wang-lst-16/-cite" class="tex hljs" data-filename="/publication/dblp-journalsjpdc-wang-lst-16/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2016">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [ICAC 2016]
            
          
        </span>
        <a href="https://doi.org/10.1109/ICAC.2016.45" class="article-title" 
          style="color:inherit; font-weight: normal;">Adaptive Power Profiling for Many-Core HPC Architectures</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-conficac-kelley-stg-16\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/ICAC.2016.45" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-conficac-kelley-stg-16\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-conficac-kelley-stg-16/-abstract" style="display: none;">
            <p class="abstract abstract-text">State of the art schedulers use workload profiles to help determine which resources to allocate. Traditionally, threads execute on every available core, but increasingly, too much power is consumed by using every core. Because peak power can occur at any point in time during the workload, workloads are commonly profiled to completion multiple times in an offline architecture. In practice, this process is too time consuming for online profiling and alternate approaches are used, such as profiling for k% of the workload or predicting peak power from similar workloads. We studied the effectiveness of these methods for core scaling. Core scaling is a technique which executes threads on a subset of available cores, allowing unused cores to enter low-power operating modes. Schedulers can use core scaling to reduce peak power, but must have an accurate profile across potential settings for number of active cores in order to know when to make this decision. We devised an accurate, fast and adaptive approach to profile peak power under core scaling. Our approach uses short profiling runs to collect instantaneous power traces for a workload under each core scaling setting. The duration of profiling varies for each power trace and depends on the desired accuracy. Compared to k% profiling of peak power, our approach reduced the profiling duration by up to 93% while keeping accuracy within 3%.</p>
          </div>
          
          
          <div id="/publication/dblp-conficac-kelley-stg-16/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-conficac-kelley-stg-16/-cite" class="tex hljs" data-filename="/publication/dblp-conficac-kelley-stg-16/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2015">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [HPCA 2015]
            
          
        </span>
        <a href="https://doi.org/10.1109/HPCA.2015.7056044" class="article-title" 
          style="color:inherit; font-weight: normal;">Understanding GPU errors on large-scale HPC systems and the implications for system design and operation</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confhpca-tiwari-grmrvoldn-15\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/HPCA.2015.7056044" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confhpca-tiwari-grmrvoldn-15\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confhpca-tiwari-grmrvoldn-15/-abstract" style="display: none;">
            <p class="abstract abstract-text">Increase in graphics hardware performance and improvements in programmability has enabled GPUs to evolve from a graphics-specific accelerator to a general-purpose computing device. Titan, the world's second fastest supercomputer for open science in 2014, consists of more dum 18,000 GPUs that scientists from various domains such as astrophysics, fusion, climate, and combustion use routinely to run large-scale simulations. Unfortunately, while the performance efficiency of GPUs is well understood, their resilience characteristics in a large-scale computing system have not been fully evaluated. We present a detailed study to provide a thorough understanding of GPU errors on a large-scale GPU-enabled system. Our data was collected from the Titan supercomputer at the Oak Ridge Leadership Computing Facility and a GPU cluster at the Los Alamos National Laboratory. We also present results from our extensive neutron-beam tests, conducted at Los Alamos Neutron Science Center (LANSCE) and at ISIS (Rutherford Appleron Laboratories, UK), to measure the resilience of different generations of GPUs. We present several findings from our field data and neutron-beam experiments, and discuss the implications of our results for future GPU architects, current and future HPC computing facilities, and researchers focusing on GPU resilience.</p>
          </div>
          
          
          <div id="/publication/dblp-confhpca-tiwari-grmrvoldn-15/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confhpca-tiwari-grmrvoldn-15/-cite" class="tex hljs" data-filename="/publication/dblp-confhpca-tiwari-grmrvoldn-15/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2015">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [DSN 2015]
            
          
        </span>
        <a href="https://doi.org/10.1109/DSN.2015.52" class="article-title" 
          style="color:inherit; font-weight: normal;">Understanding and Exploiting Spatial Properties of System Failures on Extreme-Scale HPC Systems</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confdsn-gupta-tjrm-15\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/DSN.2015.52" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confdsn-gupta-tjrm-15\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confdsn-gupta-tjrm-15/-abstract" style="display: none;">
            <p class="abstract abstract-text">As we approach exascale, the scientific simulations are expected to experience more interruptions due to increased system failures. Designing better HPC resilience techniques requires understanding the key characteristics of system failures on these systems. While temporal properties of system failures on HPC systems have been well-investigated, there is limited understanding about the spatial characteristics of system failures and its impact on the resilience mechanisms. Therefore, we examine the spatial characteristics and behavior of system failures. We investigate the interaction between spatial and temporal characteristics of failures and its implications for system operations and resilience mechanisms on large-scale HPC systems. We show that system failures have 'spatial locality' at different granularity in the system, study impact of different failure-types, and investigate the correlation among different failure-types. Finally, we propose a novel scheme that exploits the spatial locality in failures to improve application and system performance. Our evaluation shows that the proposed scheme significantly improves the system performance in a dynamic and production-level HPC system.</p>
          </div>
          
          
          <div id="/publication/dblp-confdsn-gupta-tjrm-15/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confdsn-gupta-tjrm-15/-cite" class="tex hljs" data-filename="/publication/dblp-confdsn-gupta-tjrm-15/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2015">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [SC 2015 (a)]
            
          
        </span>
        <a href="https://doi.org/10.1145/2807591.2807666" class="article-title" 
          style="color:inherit; font-weight: normal;">Reliability lessons learned from GPU experience with the Titan supercomputer at Oak Ridge leadership computing facility</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confsc-tiwari-ggrm-15\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1145/2807591.2807666" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confsc-tiwari-ggrm-15\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confsc-tiwari-ggrm-15/-abstract" style="display: none;">
            <p class="abstract abstract-text">The high computational capability of graphics processing units (GPUs) is enabling and driving the scientific discovery process at large-scale. The world's second fastest supercomputer for open science, Titan, has more than 18,000 GPUs that computational scientists use to perform scientific simulations and data analysis. Understanding of GPU reliability characteristics, however, is still in its nascent stage since GPUs have only recently been deployed at large-scale. This paper presents a detailed study of GPU errors and their impact on system operations and applications, describing experiences with the 18,688 GPUs on the Titan supercomputer as well as lessons learned in the process of efficient operation of GPUs at scale. These experiences are helpful to HPC sites which already have large-scale GPU clusters or plan to deploy GPUs in the future.</p>
          </div>
          
          
          <div id="/publication/dblp-confsc-tiwari-ggrm-15/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confsc-tiwari-ggrm-15/-cite" class="tex hljs" data-filename="/publication/dblp-confsc-tiwari-ggrm-15/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2015">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [CORR 2015]
            
          
        </span>
        <a href="https://doi.org/10.1109/ICAC.2015.33" class="article-title" 
          style="color:inherit; font-weight: normal;">Measuring and Managing Answer Quality for Online Data-Intensive Services</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-conficac-kelley-smthe-15\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/ICAC.2015.33" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-conficac-kelley-smthe-15\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-conficac-kelley-smthe-15/-abstract" style="display: none;">
            <p class="abstract abstract-text">Online data-intensive services parallelize query execution across distributed software components. Interactive response time is a priority, so online query executions return answers without waiting for slow running components to finish. However, data from these slow components could lead to better answers. We propose Ubora, an approach to measure the effect of slow running components on the quality of answers. Ubora randomly samples online queries and executes them twice. The first execution elides data from slow components and provides fast online answers, the second execution waits for all components to complete. Ubora uses memoization to speed up mature executions by replaying network messages exchanged between components. Our systems-level implementation works for a wide range of platforms, including Hadoop/Yarn, Apache Lucene, the Easy Rec Recommendation Engine, and the Open Ephyra question answering system. Ubora computes answer quality much faster than competing approaches that do not use memoization. With Ubora, we show that answer quality can and should be used to guide online admission control. Our adaptive controller processed 37% more queries than a competing controller guided by the rate of timeouts.</p>
          </div>
          
          
          <div id="/publication/dblp-conficac-kelley-smthe-15/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-conficac-kelley-smthe-15/-cite" class="tex hljs" data-filename="/publication/dblp-conficac-kelley-smthe-15/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2015">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [DSDIS 2015]
            
          
        </span>
        <a href="https://doi.org/10.1109/DSDIS.2015.66" class="article-title" 
          style="color:inherit; font-weight: normal;">Low Power Job Scheduler for Supercomputers: A Rule-Based Power-Aware Scheduler</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confdsdis-wang-tw-15\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/DSDIS.2015.66" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confdsdis-wang-tw-15\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confdsdis-wang-tw-15/-abstract" style="display: none;">
            <p class="abstract abstract-text">Supercomputer's fast processing speed provides a great convenience to the scientists who dealing with extremely large data sets. The next generation of "exascale" supercomputers could provide accurate simulation results in the area of automobile industry, aerospace and even nuclear fusion reactors for the very first time. However, the energy cost of super-computing is "super" expensive with a total electricity bill of 9 million dollars per year. Thus, Conserving energy or increase the energy efficiency are becoming more critical. Many researchers are looking into this problem and try to conserve energy by incorporating DVFS technique into their specific methods. However, this approach is limited especially when the workload is high. In this paper, we developed a power-aware job scheduler by applying rule based control method as well as real power and speedup profiles to improve power efficiency while maintain the power constraints. The intensive simulation results shown that our proposed method is able to achieve the maximum utilization of co   mputing resources, in the meantime, keep the energy cost under the threshold. Moreover, by introducing a Power Performance Factor (PPF) based on the real power and speedup profiles, we are able to increase the power efficiency up to 75%.</p>
          </div>
          
          
          <div id="/publication/dblp-confdsdis-wang-tw-15/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confdsdis-wang-tw-15/-cite" class="tex hljs" data-filename="/publication/dblp-confdsdis-wang-tw-15/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2015">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [LCTES 2015]
            
          
        </span>
        <a href="https://doi.org/10.1145/2670529.2754959" class="article-title" 
          style="color:inherit; font-weight: normal;">Clover: Compiler Directed Lightweight Soft Error Resilience</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-conflctrts-liu-jlt-15\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1145/2670529.2754959" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-conflctrts-liu-jlt-15\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-conflctrts-liu-jlt-15/-abstract" style="display: none;">
            <p class="abstract abstract-text">This paper presents Clover, a compiler directed soft error detection and recovery scheme for lightweight soft error resilience. The compiler carefully generates soft error tolerant code based on idempotent processing without explicit checkpoint. During program execution, Clover relies on a small number of acoustic wave detectors deployed in the processor to identify soft errors by sensing the wave made by a particle strike. To cope with DUE (detected unrecoverable errors) caused by the sensing latency of error detection, Clover leverages a novel selective instruction duplication technique called tail-DMR (dual modular redundancy). Once a soft error is detected by either the sensor or the tail-DMR, Clover takes care of the error as in the case of exception handling. To recover from the error, Clover simply redirects program control to the beginning of the code region where the error is detected. The experiment results demonstrate that the average runtime overhead is only 26%, which is a 75% reduction compared to that of the state-of-the-art soft error resilience technique.</p>
          </div>
          
          
          <div id="/publication/dblp-conflctrts-liu-jlt-15/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-conflctrts-liu-jlt-15/-cite" class="tex hljs" data-filename="/publication/dblp-conflctrts-liu-jlt-15/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2015">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [SC 2015 (b)]
            
          
        </span>
        <a href="https://doi.org/10.1145/2807591.2807622" class="article-title" 
          style="color:inherit; font-weight: normal;">AnalyzeThis: an analysis workflow-aware storage system</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confsc-sim-kvtabr-15\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1145/2807591.2807622" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confsc-sim-kvtabr-15\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confsc-sim-kvtabr-15/-abstract" style="display: none;">
            <p class="abstract abstract-text">The need for novel data analysis is urgent in the face of a data deluge from modern applications. Traditional approaches to data analysis incur significant data movement costs, moving data back and forth between the storage system and the processor. Emerging Active Flash devices enable processing on the flash, where the data already resides. An array of such Active Flash devices allows us to revisit how analysis workflows interact with storage systems. By seamlessly blending together the flash storage and data analysis, we create an analysis workflow-aware storage system, AnalyzeThis. Our guiding principle is that analysis-awareness be deeply ingrained in each and every layer of the storage, elevating data analyses as first-class citizens, and transforming AnalyzeThis into a potent analytics-aware appliance. We implement the AnalyzeThis storage system atop an emulation platform of the Active Flash array. Our results indicate that AnalyzeThis is viable, expediting workflow execution and minimizing data movement.</p>
          </div>
          
          
          <div id="/publication/dblp-confsc-sim-kvtabr-15/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confsc-sim-kvtabr-15/-cite" class="tex hljs" data-filename="/publication/dblp-confsc-sim-kvtabr-15/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2015">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [SC 2015 (c)]
            
          
        </span>
        <a href="https://doi.org/10.1145/2807591.2807615" class="article-title" 
          style="color:inherit; font-weight: normal;">A practical approach to reconciling availability, performance, and capacity in provisioning extreme-scale storage systems</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confsc-wan-wotvc-15\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1145/2807591.2807615" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confsc-wan-wotvc-15\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confsc-wan-wotvc-15/-abstract" style="display: none;">
            <p class="abstract abstract-text">The increasing data demands from high-performance computing applications significantly accelerate the capacity, capability and reliability requirements of storage systems. As systems scale, component failures and repair times increase, significantly impacting data availability. A wide array of decision points must be balanced in designing such systems.
 We propose a systematic approach that balances and optimizes both initial and continuous spare provisioning based on a detailed investigation of the anatomy and field failure data analysis of extreme-scale storage systems. We consider the component failure characteristics and its cost and impact at the system level simultaneously. We build a tool to evaluate different provisioning schemes, and the results demonstrate that our optimized provisioning can reduce the duration of data unavailability by as much as 52% under a fixed budget. We also observe that non-disk components have much higher failure rates than disks, and warrant careful considerations in the overall provisioning process.</p>
          </div>
          
          
          <div id="/publication/dblp-confsc-wan-wotvc-15/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confsc-wan-wotvc-15/-cite" class="tex hljs" data-filename="/publication/dblp-confsc-wan-wotvc-15/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2014">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [IPDPS 2014]
            
          
        </span>
        <a href="https://doi.org/10.1109/IPDPS.2014.18" class="article-title" 
          style="color:inherit; font-weight: normal;">MapReuse: Reusing Computation in an In-Memory MapReduce System</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confipps-tiwari-s-14\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/IPDPS.2014.18" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confipps-tiwari-s-14\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confipps-tiwari-s-14/-abstract" style="display: none;">
            <p class="abstract abstract-text">MapReduce programming model is being increasingly adopted for data intensive high performance computing. Recently, it has been observed that in data-intensive environment, programs are often run multiple times with either identical or slightly-changed input, which creates a significant opportunity for computation reuse. Recognizing the opportunity, researchers have proposed techniques to reuse computation in disk-based MapReduce systems such as Hadoop, but not for in-memory MapReduce (IMMR) systems such as Phoenix. In this paper, we propose a novel technique for computation reuse in IMMR systems, which we refer to as MapReuse. MapReuse detects input similarity by comparing their signatures. It skips re-computing output from a repeated portion of the input, computes output from a new portion of input, and removes output that corresponds to a deleted portion of the input. MapReuse is built on top of an existing IMMR system, leaving it largely unmodified. MapReuse significantly speeds up IMMR, even when the new input differs by 25% compared to the original input. </p>
          </div>
          
          
          <div id="/publication/dblp-confipps-tiwari-s-14/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confipps-tiwari-s-14/-cite" class="tex hljs" data-filename="/publication/dblp-confipps-tiwari-s-14/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2014">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [DSN 2014]
            
          
        </span>
        <a href="https://doi.org/10.1109/DSN.2014.101" class="article-title" 
          style="color:inherit; font-weight: normal;">Lazy Checkpointing: Exploiting Temporal Locality in Failures to Mitigate Checkpointing Overheads on Extreme-Scale Systems</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confdsn-tiwari-gv-14\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/DSN.2014.101" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confdsn-tiwari-gv-14\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confdsn-tiwari-gv-14/-abstract" style="display: none;">
            <p class="abstract abstract-text">Continuing increase in the computational power of supercomputers has enabled large-scale scientific applications in the areas of astrophysics, fusion, climate and combustion to run larger and longer-running simulations, facilitating deeper scientific insights. However, these long-running simulations are often interrupted by multiple system failures. Therefore, these applications rely on "check pointing" as a resilience mechanism to store application state to permanent storage and recover from failures. Unfortunately, check pointing incurs excessive I/O overhead on supercomputers due to large size of checkpoints, resulting in a sub-optimal performance and resource utilization. In this paper, we devise novel mechanisms to show how check pointing overhead can be mitigated significantly by exploiting the temporal characteristics of system failures. We provide new insights and detailed quantitative understanding of the check pointing overheads and trade-offs on large-scale machines. Our prototype implementation shows the viability of our approach on extreme-scale machines.</p>
          </div>
          
          
          <div id="/publication/dblp-confdsn-tiwari-gv-14/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confdsn-tiwari-gv-14/-cite" class="tex hljs" data-filename="/publication/dblp-confdsn-tiwari-gv-14/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2014">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [ICPADS 2014]
            
          
        </span>
        <a href="https://doi.org/10.1109/PADSW.2014.7097866" class="article-title" 
          style="color:inherit; font-weight: normal;">Improving large-scale storage system performance via topology-aware and balanced data placement</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-conficpads-wang-ogtv-14\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/PADSW.2014.7097866" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-conficpads-wang-ogtv-14\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-conficpads-wang-ogtv-14/-abstract" style="display: none;">
            <p class="abstract abstract-text">With the advent of big data, the I/O subsystems of large-scale compute clusters are becoming a center of focus. More applications are putting greater demands on end-to-end I/O performance. These subsystems are often complex in design. They comprise of multiple hardware and software layers to cope with the increasing capacity, capability, and scalability requirements of data intensive applications. However, the sharing nature of storage resources and the intrinsic interactions across these layers make it a great challenge to realize end-to-end performance gains. This paper proposes a topology-aware strategy to balance the load across resources, to improve the per-application I/O performance. We demonstrate the effectiveness of our algorithm on an extreme-scale compute cluster, Titan, at the Oak Ridge Leadership Computing Facility (OLCF). Our experiments with both synthetic benchmarks and a real-world application show that, even under congestion, our proposed algorithm can improve large-scale application I/O performance significantly, resulting in both a reduction in application run time as well as a higher resolution of simulation run.</p>
          </div>
          
          
          <div id="/publication/dblp-conficpads-wang-ogtv-14/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-conficpads-wang-ogtv-14/-cite" class="tex hljs" data-filename="/publication/dblp-conficpads-wang-ogtv-14/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2014">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [SC 2014]
            
          
        </span>
        <a href="https://doi.org/10.1109/SC.2014.23" class="article-title" 
          style="color:inherit; font-weight: normal;">Best Practices and Lessons Learned from Deploying and Operating Large-Scale Data-Centric Parallel File Systems</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confsc-oral-shlwemfgkgtvrdsb-14\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/SC.2014.23" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confsc-oral-shlwemfgkgtvrdsb-14\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confsc-oral-shlwemfgkgtvrdsb-14/-abstract" style="display: none;">
            <p class="abstract abstract-text">The Oak Ridge Leadership Computing Facility (OLCF) has deployed multiple large-scale parallel file systems (PFS) to support its operations. During this process, OLCF acquired significant expertise in large-scale storage system design, file system software development, technology evaluation, benchmarking, procurement, deployment, and operational practices. Based on the lessons learned from each new PFS deployment, OLCF improved its operating procedures, and strategies. This paper provides an account of our experience and lessons learned in acquiring, deploying, and operating large-scale parallel file systems. We believe that these lessons will be useful to the wider HPC community.</p>
          </div>
          
          
          <div id="/publication/dblp-confsc-oral-shlwemfgkgtvrdsb-14/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confsc-oral-shlwemfgkgtvrdsb-14/-cite" class="tex hljs" data-filename="/publication/dblp-confsc-oral-shlwemfgkgtvrdsb-14/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2013">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [FAST 2013]
            
          
        </span>
        <a href="https://www.usenix.org/conference/fast13/technical-sessions/presentation/tiwari" class="article-title" 
          style="color:inherit; font-weight: normal;">Active flash: towards energy-efficient, in-situ data analytics on extreme-scale machines</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-conffast-tiwari-bvkmds-13\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://www.usenix.org/conference/fast13/technical-sessions/presentation/tiwari" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-conffast-tiwari-bvkmds-13\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-conffast-tiwari-bvkmds-13/-abstract" style="display: none;">
            <p class="abstract abstract-text">Modern scientiﬁc discovery is increasingly driven by large-scale supercomputing simulations, followed by data analysis tasks. These data analyses are either performed ofﬂine, on smaller-scale clusters, or on the supercomputer itself. Unfortunately, these techniques suffer from performance and energy inefﬁciencies due to increased data movement between the compute and storage subsystems. Therefore, we propose Active Flash, an insitu scientiﬁc data analysis approach, wherein data analysis is conducted on the solid-state device (SSD), wherethe data already resides. Our performance and energy models show that Active Flash has the potential to address many of the aforementioned concerns without degrading HPC simulation performance. In addition, we demonstrate an Active Flash prototype built on a commercial SSD controller, which further reafﬁrms the viability of our proposal.</p>
          </div>
          
          
          <div id="/publication/dblp-conffast-tiwari-bvkmds-13/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-conffast-tiwari-bvkmds-13/-cite" class="tex hljs" data-filename="/publication/dblp-conffast-tiwari-bvkmds-13/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2012">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [OSDI 2012]
            
          
        </span>
        <a href="https://www.usenix.org/conference/hotpower12/workshop-program/presentation/tiwari" class="article-title" 
          style="color:inherit; font-weight: normal;">Reducing Data Movement Costs Using Energy-Efficient, Active Computation on SSD</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confosdi-tiwari-vkmbd-12\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://www.usenix.org/conference/hotpower12/workshop-program/presentation/tiwari" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confosdi-tiwari-vkmbd-12\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confosdi-tiwari-vkmbd-12/-abstract" style="display: none;">
            <p class="abstract abstract-text"> Modern scientific discovery often involves running complex application simulations on supercomputers, followed by a sequence of data analysis tasks on smaller clusters. This offline approach suffers from significant data movement costs such as redundant I/O, storage bandwidth bottleneck, and wasted CPU cycles, all of which contribute to increased energy consumption and delayed end-to- end performance. Technology projections for an exascale machine indicate that energy-efficiency will become the primary design metric. It is estimated that the energy cost of data movement will soon rival the cost of computation. Consequently, we can no longer ignore the data movement costs in data analysis.<br/><br/> To address these challenges, we advocate executing data analysis tasks on emerging storage devices, such as SSDs. Typically, in extreme-scale systems, SSDs serve only as a temporary storage system for the simulation output data. In our approach, Active Flash, we propose to conduct in-situ data analysis on the SSD controller without degrading the performance of the simulation job. By migrating analysis tasks closer to where the data resides, it helps reduce the data movement cost. We present detailed energy and performance models for both active flash and offline strategies, and study them using extreme-scale application simulations, commonly used data analytics kernels, and supercomputer system configurations. Our evaluation suggests that active flash is a promising approach to alleviate the storage bandwidth bottleneck, reduce the data movement cost, and improve the overall energy efficiency.</p>
          </div>
          
          
          <div id="/publication/dblp-confosdi-tiwari-vkmbd-12/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confosdi-tiwari-vkmbd-12/-cite" class="tex hljs" data-filename="/publication/dblp-confosdi-tiwari-vkmbd-12/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2012">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [IPDPS 2012]
            
          
        </span>
        <a href="https://doi.org/10.1109/IPDPS.2012.119" class="article-title" 
          style="color:inherit; font-weight: normal;">Modeling and Analyzing Key Performance Factors of Shared Memory MapReduce</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confipps-tiwari-s-12\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/IPDPS.2012.119" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confipps-tiwari-s-12\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confipps-tiwari-s-12/-abstract" style="display: none;">
            <p class="abstract abstract-text">MapReduce parallel programming model has seen wide adoption in data center applications. Recently, lightweight, fast, in-memory MapReduce runtime systems have been proposed for shared memory systems. However, what factors affect performance and what performance bottlenecks exist for a given program, are not well understood. This paper builds an analytical model to capture key performance factors of shared memory MapReduce and investigates important performance trends and behavior. Our study discovers several important findings and implications for system designers, performance tuners, and programmers. Our model quantifies relative contribution of different key performance factors for both map and reduce phases, and shows that performance of MapReduce programs are highly input-content dependent. Our model reveals that performance is heavily affected by the order in which distinct keys are encountered during the Map phase, and the frequency of these distinct keys. Our model points out cases in which reduce phase time dominates the total execution time. We also show that data-structure and algorithm design choices affect map and reduce phases differently and sometimes affecting map phase positively while affecting reduce phase negatively. Finally, we propose an application classification framework that can be used to reason about performance bottlenecks for a given application.</p>
          </div>
          
          
          <div id="/publication/dblp-confipps-tiwari-s-12/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confipps-tiwari-s-12/-cite" class="tex hljs" data-filename="/publication/dblp-confipps-tiwari-s-12/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2012">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [ISPASS 2012]
            
          
        </span>
        <a href="https://doi.org/10.1109/ISPASS.2012.6189228" class="article-title" 
          style="color:inherit; font-weight: normal;">Architectural characterization and similarity analysis of sunspider and Google&#39;s V8 Javascript benchmarks</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confispass-tiwari-s-12\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/ISPASS.2012.6189228" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confispass-tiwari-s-12\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confispass-tiwari-s-12/-abstract" style="display: none;">
            <p class="abstract abstract-text">Today, more than 99% of web-browsers are enabled with Javascript capabilities, and Javascript's popularity is only going to increase in the future. However, due to bytecode interpretation, Javascript codes suffer from severe performance penalty (up to 50x slower) compared to the corresponding native C/C++ code. We recognize that the first step to bridge this performance gap is to understand the the architectural execution characteristics of Javascript benchmarks. Therefore, this paper presents an in-depth architectural characterization of widely used V8 and Sunspider Javascript benchmarks using Google's V8 javascript engine. Using statistical data analysis techniques, our characterization study discovers and explains correlation among different execution characteristics in microarchitecture dependent as well as microarchitecture independent fashion. Furthermore, our study measures (dis)similarity among 33 different Javascript benchmarks and discusses its implications. Given the widespread use of Javascripts, we believe our findings are useful for both performance analysis and benchmarking communities.</p>
          </div>
          
          
          <div id="/publication/dblp-confispass-tiwari-s-12/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confispass-tiwari-s-12/-cite" class="tex hljs" data-filename="/publication/dblp-confispass-tiwari-s-12/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2011">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [HPCS 2011]
            
          
        </span>
        <a href="https://doi.org/10.1109/HPCA.2011.5749720" class="article-title" 
          style="color:inherit; font-weight: normal;">HAQu: Hardware-accelerated queueing for fine-grained threading on a chip multiprocessor</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confhpca-lee-tst-11\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/HPCA.2011.5749720" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confhpca-lee-tst-11\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confhpca-lee-tst-11/-abstract" style="display: none;">
            <p class="abstract abstract-text">Queues are commonly used in multithreaded programs for synchronization and communication. However, because software queues tend to be too expensive to support finegrained parallelism, hardware queues have been proposed to reduce overhead of communication between cores. Hardware queues require modifications to the processor core and need a custom interconnect. They also pose difficulties for the operating system because their state must be preserved across context switches. To solve these problems, we propose a hardware-accelerated queue, or HAQu. HAQu adds hardware to a CMP that accelerates operations on software queues. Our design implements fast queueing through an application's address space with operations that are compatible with a fully software queue. Our design provides accelerated and OS-transparent performance in three general ways: (1) it provides a single instruction for enqueueing and dequeueing which significantly reduces the overhead when used in fine-grained threading; (2) operations on the queue are designed to leverage low-level details of the coherence protocol; and (3) hardware ensures that the full state of the queue is stored in the application's address space, thereby ensuring virtualization. We have evaluated our design in the context of application domains: offloading fine-grained checks for improved software reliability, and automatic, fine-grained parallelization using decoupled software pipelining.</p>
          </div>
          
          
          <div id="/publication/dblp-confhpca-lee-tst-11/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confhpca-lee-tst-11/-cite" class="tex hljs" data-filename="/publication/dblp-confhpca-lee-tst-11/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        

        
        
        

        

        <div class="grid-sizer col isotope-item pubtype-1 year-2010">
          
          
          














<div class="view-list-item">
  <div class="container">
    <div class="row">
      <div class="col">
        <span style="
        color:maroon;
        font-weight: bold;">
          
          
            
            
              
              [IPDPS 2010]
            
          
        </span>
        <a href="https://doi.org/10.1109/IPDPS.2010.5470428" class="article-title" 
          style="color:inherit; font-weight: normal;">MMT: Exploiting fine-grained parallelism in dynamic memory management</a>
        

        

        

        
        <div class="btn-links">
          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_visibility('\/publication\/dblp-confipps-tiwari-lts-10\/')">
            Abstract
          </span>

          <a class="btn btn-outline-primary my-1 mr-1 btn-md" href="https://doi.org/10.1109/IPDPS.2010.5470428" target="_blank" rel="noopener">
            Paper
          </a>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Presentation
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md">
            Open-Source Artifact
          </span>

          <span class="btn btn-outline-primary my-1 mr-1 btn-md" onclick="toggle_cite('\/publication\/dblp-confipps-tiwari-lts-10\/')">
            BibTex
          </span>

        </div>
        
        <div>
          <div id="/publication/dblp-confipps-tiwari-lts-10/-abstract" style="display: none;">
            <p class="abstract abstract-text">Dynamic memory management is one of the most expensive but ubiquitous operations in many C/C++ applications. Additional features such as security checks, while desirable, further worsen memory management overheads. With advent of multicore architecture, it is important to investigate how dynamic memory management overheads for sequential applications can be reduced. In this paper, we propose a new approach for accelerating dynamic memory management on multicore architecture, by offloading dynamic management functions to a separate thread that we refer to as memory management thread (MMT). We show that an efficient MMT design can give significant performance improvement by extracting parallelism while being agnostic to the underlying memory management library algorithms and data structures. We also show how parallelism provided by MMT can be beneficial for high overhead memory management tasks, for example, security checks related to memory management. We evaluate MMT on heap allocation-intensive benchmarks running on an Intel core 2 quad platform for two widely-used memory allocators: Doug Lea's and PHKmalloc allocators. On average, MMT achieves a speedup ratio of 1.19× for both allocators, while both the application and memory management libraries are unmodified and are oblivious to the parallelization scheme. For PHKmalloc with security checks turned on, MMT reduces the security check overheads from 21% to only 1% on average.</p>
          </div>
          
          
          <div id="/publication/dblp-confipps-tiwari-lts-10/-cite-div" style="display: none;">
            <pre>
              <code id="/publication/dblp-confipps-tiwari-lts-10/-cite" class="tex hljs" data-filename="/publication/dblp-confipps-tiwari-lts-10/cite.bib"></code>
            </pre>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</div>
          
        </div>

        
      </div>

    </div>
  </div>
</div>
<script>
  function toggle_visibility(id) {
    let e = document.getElementById(id + "-abstract");
    let citeDiv = document.getElementById(id + "-cite-div");
    let $grid_pubs = $('#container-publications');
    if (e.style.display == 'block') {
      e.style.display = 'none';
      $grid_pubs.isotope('layout')
    }
    else {
      e.style.display = 'block';
      citeDiv.style.display = 'none';
      $grid_pubs.isotope('layout')
    }
  }

  function toggle_cite(id) {
    let $grid_pubs = $('#container-publications');
    let abstractDiv = document.getElementById(id + "-abstract");
    let citeDiv = document.getElementById(id + "-cite-div");
    let citeTarget = document.getElementById(id + "-cite");
    let $citeTarget = $(citeTarget);
    let filename = $citeTarget.attr('data-filename');
    if (citeDiv.style.display == 'block') {
      citeDiv.style.display = 'none';
      $grid_pubs.isotope('layout')
    }
    else {
      citeDiv.style.display = 'block';
      abstractDiv.style.display = 'none';
      $citeTarget.load(filename, function (resolve, reject) {
        $grid_pubs.isotope('layout')
      })
    }
  }
</script>
<style>

  .abstract {
    font-size: 16px;
    margin-top: 10px;
  }

  .header {
    padding-left: 10%;
    padding-bottom: 15px;
    padding-top: 2px;
    margin-top: 0;
    margin-bottom: 30px;
    margin-right: 0;
    background-color: maroon;
  }

  .header>h1 {
    color: white;
    font-size: 1.75rem;
  }

  .row {
    margin-right: 0;
  }
</style>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks",
        'slides' : "Slides"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.66c553246b0f279a03be6e5597f72b52.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
